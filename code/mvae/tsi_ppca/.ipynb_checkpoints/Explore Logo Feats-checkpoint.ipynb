{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "DECODER_DIMS = {\"text\": 400, \"bin\": 400, \"cat\": 400, \"bp\": 400, \"indus\": 400}\n",
    "ENCODER_DIMS = {\"full\": 400, \"res\": 200, \"mgr\": 200, \"design\": 200}\n",
    "\n",
    "K = 20\n",
    "\n",
    "FOLDS = 4\n",
    "BATCHES = 4000\n",
    "ITERS = 10\n",
    "\n",
    "ADAM_LR = 0.00001\n",
    "MIN_AF = 1e-6\n",
    "ANNEALING_BATCHES = 3500\n",
    "NUM_PARTICLES = 1\n",
    "\n",
    "CENTER_BP = True\n",
    "\n",
    "WEIGHT_DECAY = 0.\n",
    "\n",
    "DISABLE_TQDM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.examples.util  # patches torchvision\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro import poutine\n",
    "\n",
    "pyro.set_rng_seed(42)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from skimage import io\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from data import SplitData\n",
    "from model import LogoMVAE\n",
    "\n",
    "assert pyro.__version__.startswith('1.3.0')\n",
    "\n",
    "\n",
    "\n",
    "# # Helper functions:\n",
    "\n",
    "\n",
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "\n",
    "def compute_distance(z):\n",
    "    b = z.reshape(z.shape[0], 1, z.shape[1])\n",
    "    return np.sqrt(np.einsum('ijk, ijk->ij', z-b, z-b))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Data Loading\n",
    "\n",
    "# First, load text data, and apply word filter. Note on notation: `tx` stands for \"true x,\" because the model variables are also called x.\n",
    "\n",
    "textdf = pd.read_csv(\"../../../data/web_dtfm20_binary.csv\", index_col=0)\n",
    "tx_text = textdf.values\n",
    "seltext = tx_text.sum(0) > 0.05\n",
    "tx_text = textdf.values[:,seltext]\n",
    "\n",
    "gt20words = tx_text.sum(1) > 20\n",
    "tx_text = tx_text[gt20words,:]\n",
    "\n",
    "words = textdf.columns[seltext]\n",
    "N, V = tx_text.shape\n",
    "\n",
    "binfeats = pd.read_csv(\"../../../data/y_bin_all_py2.csv\", index_col=0)\n",
    "tx_b = binfeats.values\n",
    "tx_b = tx_b[gt20words,:]\n",
    "M_b = tx_b.shape[1]\n",
    "\n",
    "catfeats = pd.read_csv(\"../../../data/y_mult_ncolors_py2.csv\", index_col=0)\n",
    "\n",
    "tx_c1 = catfeats.values[:,0][gt20words]\n",
    "M_c1 = len(np.unique(tx_c1))\n",
    "tx_c1 = np.expand_dims(tx_c1, 1)\n",
    "\n",
    "tx_c2 = catfeats.values[:,1][gt20words]\n",
    "M_c2 = len(np.unique(tx_c2))\n",
    "tx_c2 = np.expand_dims(tx_c2, 1)\n",
    "\n",
    "tx_c3 = catfeats.values[:,2][gt20words]\n",
    "M_c3 = len(np.unique(tx_c3))\n",
    "tx_c3 = np.expand_dims(tx_c3, 1)\n",
    "\n",
    "tx_c4 = catfeats.values[:,3][gt20words]\n",
    "M_c4 = len(np.unique(tx_c4))\n",
    "tx_c4 = np.expand_dims(tx_c4, 1)\n",
    "\n",
    "tx_c5 = catfeats.values[:,4][gt20words]\n",
    "M_c5 = len(np.unique(tx_c5))\n",
    "tx_c5 = np.expand_dims(tx_c5, 1)\n",
    "\n",
    "c1_labels = np.array([\"black\",\"blue_dark\",\"blue_light\",\"blue_medium\",\"brown\",\"green_dark\",\n",
    "                      \"green_light\",\"grey_dark\",\"grey_light\",\"orange\",\"red\",\"red_dark\",\n",
    "                      \"yellow\"])\n",
    "\n",
    "c2_labels = np.array([\"circle\",\"rect-oval_medium\",\"rect-oval_large\",\"rect-oval_thin\",\n",
    "                      \"square\",\"triangle\"])\n",
    "\n",
    "c3_labels = np.array([\"bad_letters\",\"bulky_hollow_geometric\",\"circular\",\"dense_simple_geom\",\n",
    "                      \"detailed_circle\",\"hollow_circle\",\"detailed_hor\",\"long_hor\",\"no_mark\",\n",
    "                      \"simple\",\"square\",\"thin_vert_rect\",\"vert_narrow\",\"detailed\",\"thin\",\n",
    "                      \"hor_wispy\"])\n",
    "\n",
    "c4_labels = np.array([\"nochars\",\"sans\",\"serif\"])\n",
    "\n",
    "c5_labels = np.array([\"one_color\",\"two_colors\",\"three_colors\",\"many_colors\"])\n",
    "\n",
    "bp = pd.read_csv(\"../../../data/bp_avg_all_traits.csv\", index_col=0)\n",
    "\n",
    "bp_labels = bp.columns\n",
    "\n",
    "tx_bp = bp.values\n",
    "tx_bp = tx_bp[gt20words]\n",
    "if CENTER_BP:\n",
    "    tx_bp = (tx_bp - tx_bp.mean(0)) / tx_bp.std(0)\n",
    "M_bp = tx_bp.shape[1]\n",
    "\n",
    "indus = pd.read_csv(\"../../../data/industry_codes_b2bc.csv\", index_col=0)\n",
    "indus = indus.iloc[np.in1d(indus.index, bp.index),:]\n",
    "indus = indus.sort_index()\n",
    "\n",
    "tx_indus = indus.values.astype('int')\n",
    "tx_indus = tx_indus[:, tx_indus.sum(0) > 9]\n",
    "tx_indus = tx_indus[gt20words,:]\n",
    "M_indus = tx_indus.shape[1]\n",
    "\n",
    "indus_labels = indus.columns[indus.values.sum(0) > 9]\n",
    "\n",
    "allnames = binfeats.index.values[gt20words]\n",
    "\n",
    "x_sizes = {\"text\": V, \n",
    "           \"bin\": M_b, \n",
    "           \"cat1\": M_c1, \n",
    "           \"cat2\": M_c2, \n",
    "           \"cat3\": M_c3, \n",
    "           \"cat4\": M_c4, \n",
    "           \"cat5\": M_c5, \n",
    "           \"bp\": M_bp, \n",
    "           \"indus\": M_indus, \n",
    "           \"logo\": M_b + M_c1 + M_c2 + M_c3 + M_c4 + M_c5, \n",
    "           \"all\": V + M_b + M_c1 + M_c2 + M_c3 + M_c4 + M_c5 + M_bp + M_indus}\n",
    "\n",
    "task_sizes = {\"full\": x_sizes[\"all\"], \n",
    "              \"res\": x_sizes[\"logo\"] + x_sizes[\"indus\"], \n",
    "              \"design\": x_sizes[\"text\"] + x_sizes[\"bp\"] + x_sizes[\"indus\"], \n",
    "              \"mgr\": x_sizes[\"all\"] - x_sizes[\"bp\"]}\n",
    "\n",
    "noptions = np.array([M_c1, M_c2, M_c3, M_c4, M_c5])\n",
    "\n",
    "\n",
    "## Training: Instantiate Model and Run\n",
    "\n",
    "givens = pd.DataFrame(np.concatenate(([[K], list(DECODER_DIMS.values()), list(ENCODER_DIMS.values()), [BATCHES], [ITERS], [ADAM_LR], [ANNEALING_BATCHES], [NUM_PARTICLES], [CENTER_BP], [WEIGHT_DECAY]]))).T\n",
    "givens.columns = [\"K\", \"text_dec\", \"bin_dec\", \"cat_dec\", \"bp_dec\", \"indus_dec\", \"full_enc\", \"logo_enc\", \"mgr_enc\", \"des_enc\", \"batches\", \"iters\", \"adam_lr\", \"annealing_batches\", \"num_particles\", \"center_bp\", \"weight_decay\"]\n",
    "\n",
    "\n",
    "# Create holdout and cross-validation subsets (just the indices):\n",
    "\n",
    "if FOLDS > 1:\n",
    "    holdout_indices = list(split(np.arange(N), FOLDS))\n",
    "    holdout_indices.append(np.array([]))\n",
    "    fold_indices = [np.setdiff1d(np.arange(N), holdout_indices[i]) for i in range(FOLDS)]\n",
    "    fold_indices.append(np.arange(N))\n",
    "else:\n",
    "    holdout_indices = [np.array([])]\n",
    "    \n",
    "    \n",
    "# Set the KL annealing schedule (same across each fold):\n",
    "schedule = np.linspace(MIN_AF, 1., ANNEALING_BATCHES)\n",
    "# schedule = np.concatenate([np.linspace(MIN_AF, 1., round(ANNEALING_BATCHES/4.)) for _ in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_training = []\n",
    "track_test = []\n",
    "track_mgr_bp = []\n",
    "track_des_bin = []\n",
    "track_res_bp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0f97489760437f94dfb688b4032d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Folds', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef145d4c91fe4dbf8e366c4a06027d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=4000.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the model across all folds (sequentially):\n",
    "for fold in tqdm(range(1), desc=\"Folds\", disable=DISABLE_TQDM):\n",
    "    \n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    data = SplitData(tx_text, tx_b, tx_c1, tx_c2, tx_c3, tx_c4, tx_c5, tx_bp, tx_indus, \n",
    "                     allnames, noptions, test_indices = holdout_indices[fold])   \n",
    "    \n",
    "    has_test = hasattr(data, 'test')\n",
    "    if has_test:\n",
    "        data.test.make_torch()\n",
    "        \n",
    "    lmvae = LogoMVAE(K, ENCODER_DIMS, DECODER_DIMS, x_sizes, task_sizes, use_cuda=True)\n",
    "    scheduler = pyro.optim.ExponentialLR({'optimizer': torch.optim.Adam, 'optim_args': {'lr': 1e-7}, 'gamma': 1.1})\n",
    "    svi = SVI(lmvae.model, lmvae.guide, scheduler, loss=Trace_ELBO(num_particles = NUM_PARTICLES))\n",
    "\n",
    "    for i in tqdm(range(BATCHES), desc=\"Batches\", leave=False, disable=DISABLE_TQDM):\n",
    "\n",
    "        if i < ANNEALING_BATCHES:\n",
    "            annealing_factor = schedule[i]\n",
    "        else:\n",
    "            annealing_factor = 1.\n",
    "\n",
    "        data.training.shuffle()\n",
    "\n",
    "        for j in tqdm(range(ITERS), desc=\"Iters\", leave=False, disable=True):\n",
    "            svi.step(data.training, annealing_factor)\n",
    "            \n",
    "        if (i % 50 == 0) or (i == BATCHES-1):\n",
    "            track_training.append(svi.evaluate_loss(data.training, annealing_factor))\n",
    "            if has_test: \n",
    "                track_test.append(svi.evaluate_loss(data.test, annealing_factor))\n",
    "                \n",
    "                lmvae.eval();\n",
    "                \n",
    "                # Predictions for res task:\n",
    "                lmvae.predict(data.test, network = \"res\")\n",
    "                track_res_bp.append(lmvae.pred.metrics.bp_mse.features.mean())\n",
    "                \n",
    "                # Predictions for des task:        \n",
    "                lmvae.predict(data.test, network = \"des\")\n",
    "                track_des_bin.append(lmvae.pred.metrics.bin_report['macro avg']['f1-score'])\n",
    "                \n",
    "                # Predictions for mgr task:\n",
    "                lmvae.predict(data.test, network = \"mgr\")\n",
    "                track_mgr_bp.append(lmvae.pred.metrics.bp_mse.features.mean())\n",
    "                \n",
    "                lmvae.train();\n",
    "            \n",
    "            scheduler.step()                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro",
   "language": "python",
   "name": "pyro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
