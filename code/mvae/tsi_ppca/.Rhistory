b = 2
curve(dgamma(x, a, b), 0.0001, 10)
round(quantile(rgamma(100000, a, rate=b), c(0.001, 0.01, 0.2, 0.5, 0.8, 0.99)),3)
a = 10
b = 2
curve(dgamma(x, a, b), 0.0001, 10)
round(quantile(rgamma(100000, a, rate=b), c(0.001, 0.01, 0.2, 0.5, 0.8, 0.99)),3)
a = 5
b = 2
curve(dgamma(x, a, b), 0.0001, 10)
round(quantile(rgamma(100000, a, rate=b), c(0.001, 0.01, 0.2, 0.5, 0.8, 0.99)),3)
a = 5
b = 1
curve(dgamma(x, a, b), 0.0001, 10)
round(quantile(rgamma(100000, a, rate=b), c(0.001, 0.01, 0.2, 0.5, 0.8, 0.99)),3)
a = 2
b = 1.5
curve(dgamma(x, a, b), 0.0001, 10)
round(quantile(rgamma(100000, a, rate=b), c(0.001, 0.01, 0.2, 0.5, 0.8, 0.99)),3)
load("~/Dropbox/1_proj/via/me/SAVED_0-2_split_fl.RData")
require(data.table)
require(magrittr)
require(rstan)
require(ggplot2)
W_min = train[,min(week),id]$V1
dl = list(N = max(train$id), W = max(train$week), M = nrow(train), W_min = W_min,
id = train$id, week = train$week, dayhour = train$dayhour, y = train$y,
rho_mu = 0.45, rho_eta = 0.2)
inputs = expand.grid(0:23, 1:7)[,2:1]
alpha = array(dim = dim(fl$z_alpha))
for(i in 1:dl$N){
alpha_i = matrix(NA, 200, dl$W)
alpha_i[,1] = fl$alpha0 + fl$sigma_alpha*fl$z_alpha[,i,1]
for(w in 2:ncol(alpha_i)){
alpha_i[,w] = alpha_i[,w-1] + fl$sigma_alpha*fl$z_alpha[,i,w]
}
alpha[,i,] = alpha_i
}
alpha_hat = apply(alpha, c(2,3), mean)
mu_hat = colMeans(fl$mu)
eta_hat = apply(fl$eta, c(3,2), mean)
alpha_hat = alpha_hat + colMeans(eta_hat) + mean(mu_hat)
mu_hat = mu_hat - mean(mu_hat)
eta_hat = t(t(eta_hat) - colMeans(eta_hat))
active_inds = rowMeans(alpha_hat[,30:40]) > -6 & W_min < 30
active_ids = which(active_inds)
active_alpha = alpha_hat[active_inds,]
active_dists = as.matrix(dist(active_alpha))
active_dists[lower.tri(active_dists, diag=TRUE)] = 999
min_pairs = c()
for(j in 1:10){
pair_j = which(active_dists == sort(active_dists)[j], arr.ind = TRUE)
min_pairs = rbind(min_pairs, pair_j)
}
par(mfrow=c(2,5))
for(j in 1:10){matplot(t(active_alpha[c(min_pairs[j,]),max(W_min[min_pairs[j,]]):dl$W]), type = "l", ylab = "Alpha", xlab="Week")}
focal_ids = active_ids[min_pairs[1,]]
i1 = focal_ids[1]
i2 = focal_ids[2]
par(mfrow=c(1,1))
matplot(eta_hat[,c(i1,i2)], type="l", lwd=1)
par(mfrow=c(1,1))
plot(exp(alpha_hat[i1,40] + eta_hat[,i1]), type="l", lwd=2, col=2, ylim=c(0,1))
lines(exp(alpha_hat[i2,40] + eta_hat[,i2]), lwd=1, col=4)
lambda = exp(alpha_hat[,40] + t(eta_hat) + mu_hat)
hist(rowSums(lambda), breaks=40)
ind5_6 = which(rowSums(lambda) > 5 & rowSums(lambda) < 6)
alpha_hat[ind5_6,40]
par(mfrow=c(1,2), mai=0.3*(4:1))
matplot(30:40, t(alpha_hat[ind5_6[c(1,18)],30:40]), type = "l", ylab = "Alpha", xlab = "Week")
matplot(eta_hat[,ind5_6[c(1,18)]], type = "l", ylab = "Eta", xlab = "Day/Hour")
colSums(exp(eta_hat[,ind5_6[c(1,18)]]))
colSums(abs(eta_hat[,ind5_6[c(1,18)]]))
par(mfrow=c(1,1))
matplot(t(alpha_hat[ind5_6[c(1,18)],40] + t(eta_hat[,ind5_6[c(1,18)]])),
type = "l", xlab = "Day/Hour", ylab="Alpha + Eta")
par(mfrow=c(1,2))
matplot(exp(t(alpha_hat[ind5_6[c(1,18)],40] + t(eta_hat[,ind5_6[c(1,18)]]))),
type = "l", xlab = "Day/Hour", ylab="Exp(Alpha + Eta)")
matplot(exp(t(alpha_hat[ind5_6[c(1,18)],40] + t(eta_hat[,ind5_6[c(1,18)]]))),
type = "l", xlab = "Day/Hour", ylab="Exp(Alpha + Eta)", ylim=c(0,0.2))
par(mfrow=c(1,1))
matplot(exp(mu_hat + t(alpha_hat[ind5_6[c(1,18)],40] + t(eta_hat[,ind5_6[c(1,18)]]))),
type = "l", xlab = "Day/Hour", ylab="Exp(Mu + Alpha + Eta)")
m_abs = colSums(abs(eta_hat))
m_exp = colSums(exp(eta_hat))
ho_reqs = test[order(id),sum(y),id]
y_reqs = rep(0,dl$N)
y_reqs[ho_reqs$id] = ho_reqs$V1
post60 = test[week > 60,]
ho_reqs = post60[order(id),sum(y),id]
y_reqs_60 = rep(0,dl$N)
y_reqs_60[ho_reqs$id] = ho_reqs$V1
active_past_60 = y_reqs_60 > 0
lm_reqs = lm(y_reqs ~ alpha_hat[,40] + m_abs)
summary(lm_reqs)
lm_reqs = lm(y_reqs ~ alpha_hat[,40] + m_exp)
summary(lm_reqs)
lm_reqs60 = lm(y_reqs_60 ~ alpha_hat[,40] + m_abs)
summary(lm_reqs60)
lr_churn = glm(active_past_60 ~ alpha_hat[,40] + m_abs, family=binomial)
summary(lr_churn)
lm_reqs_lambda = lm(y_reqs ~ rowSums(lambda) + alpha_hat[,40] + m_abs)
summary(lm_reqs_lambda)
pairs(data.frame(Requests = y_reqs, Expected40 = rowSums(lambda), Alpha40 = alpha_hat[,40], RoutineAbs = m_abs))
pairs(data.frame(Requests = y_reqs, Expected40 = rowSums(lambda), ExpAlpha40 = exp(alpha_hat[,40]), RoutineAbs = m_abs))
lm_reqs_lambda = lm(y_reqs ~ scale(rowSums(lambda)) + scale(m_abs))
summary(lm_reqs_lambda)
lm_reqs_lambda = lm(y_reqs_60 ~ scale(rowSums(lambda)) + scale(m_abs))
summary(lm_reqs_lambda)
lr_churn = glm(active_past_60 ~ scale(rowSums(lambda)) + scale(m_abs), family=binomial)
summary(lr_churn)
lr_null = glm(active_past_60 ~ 1, family=binomial)
1-c(logLik(lr_churn))/c(logLik(lr_null))
lr_churn = glm(active_past_60 ~ rowSums(lambda) + m_abs, family=binomial)
summary(lr_churn)
rec = 40 - train[order(id),max(week),id]$V1
freq = c(table(train$id))
lm_reqs_rf= lm(y_reqs ~ rec + freq + scale(m_abs))
summary(lm_reqs_rf)
lm_reqs60_rf = lm(y_reqs_60 ~ rec + freq + m_abs)
summary(lm_reqs60_rf)
lr_churn_rf = glm(active_past_60 ~ rec + log(freq) + m_abs, family=binomial)
summary(lr_churn_rf)
holdout_data = data.frame(y_reqs, y_reqs_60, active_past_60, rec, freq, m_abs, lambda = rowSums(lambda), alpha = alpha_hat[,40])
active_data = holdout_data[holdout_data$rec == 0,]
lm_reqs60_rf = lm(y_reqs_60 ~ alpha + lambda + freq + m_abs, data = active_data)
summary(lm_reqs60_rf)
lr_churn_rf = glm(active_past_60 ~ freq + m_abs, family=binomial, data=active_data)
summary(lr_churn_rf)
hist(m_abs)
hist(m_abs, breaks=20)
hist(m_abs, breaks=20, xlab = "Routineness (r)", prob = TRUE)
hist(m_abs, breaks=20, xlab = "Routineness (r)")
hist(m_abs, breaks=20, xlab = "Routineness (r)", col = "goldenrod1")
hist(m_abs, breaks=20, xlab = "Routineness (r)", col = "goldenrod1", main="Distribution of Routineness")
hist(m_abs, breaks=20, xlab = "Routineness (r)", col = "steelblue", main="Distribution of Routineness")
hist(m_abs, breaks=20, xlab = "Routineness (r)", col = rgb(0,0,1,0.2), main="Distribution of Routineness")
hist(m_abs, breaks=20, xlab = "Routineness (r)", col = rgb(0,0.2,1,0.2), main="Distribution of Routineness")
?rgb
hist(m_abs, breaks=20, xlab = "Routineness (r)", col = rgb(0,0.2,1,0.2), main="Distribution of Routineness")
hist(m_abs, breaks=20, xlab = "Routineness (r)", col = rgb(0,0.5,1,0.2), main="Distribution of Routineness")
hist(m_abs, breaks=20, xlab = "Routineness (r)", col = rgb(0.1,0.5,1,0.2), main="Distribution of Routineness")
hist(m_abs, breaks=20, xlab = "Routineness (r)", col = rgb(0.2,0.5,1,0.2), main="Distribution of Routineness")
hist(m_abs, breaks=20, xlab = "Routineness (r)", col = rgb(0.3,0.5,1,0.2), main="Distribution of Routineness")
hist(m_abs, breaks=20, xlab = "Routineness (r)", col = rgb(0.3,0.8,1,0.2), main="Distribution of Routineness")
hist(m_abs, breaks=20, xlab = "Routineness (r)", col = rgb(0.3,0.6,1,0.2), main="Distribution of Routineness")
start = 50
end = 60
slice_size = end - start + 1
start = 50
end = 60
slice_size = end - start + 1
for(i in 1:slice_size){
print(start + i - 1)
}
rstan:::rstudio_stanc("Dropbox/1_proj/via/me/code/stan/rw_fixls_reduce_sum.stan")
rstan:::rstudio_stanc("Dropbox/1_proj/via/me/code/stan/rw_fixls_reduce_sum.stan")
rstan:::rstudio_stanc("Dropbox/1_proj/via/me/code/stan/rw_fixls_reduce_sum.stan")
rstan:::rstudio_stanc("Dropbox/1_proj/via/me/code/stan/rw_fixls_reduce_sum.stan")
rstan:::rstudio_stanc("Dropbox/1_proj/via/me/code/stan/rw_fixls_reduce_sum.stan")
rstan:::rstudio_stanc("Dropbox/1_proj/via/me/code/stan/rw_fixls_reduce_sum.stan")
install.packages(c("data.table", "ggplot2", "rstan", "magrittr"))
rstan:::rstudio_stanc("Dropbox/1_proj/via/me/code/stan/rw_fixls_reduce_sum.stan")
rstan:::rstudio_stanc("Dropbox/1_proj/via/me/code/stan/rw_fixls_reduce_sum.stan")
rstan:::rstudio_stanc("Dropbox/1_proj/via/me/code/stan/Via Model 1.1 - RW, Fixed LS, Sum Reduce.stan")
rstan:::rstudio_stanc("Dropbox/1_proj/via/me/code/stan/Via Model 1.1 - RW, Fixed LS, Sum Reduce.stan")
rstan:::rstudio_stanc("Dropbox/1_proj/via/me/code/stan/Via Model 1.1 - RW, Fixed LS, Sum Reduce.stan")
rstan:::rstudio_stanc("Dropbox/1_proj/via/me/code/stan/Via Model 1.1 - RW, Fixed LS, Sum Reduce.stan")
rstan:::rstudio_stanc("Dropbox/1_proj/via/me/code/stan/Via Model 1.1 - RW, Fixed LS, Sum Reduce.stan")
rstan:::rstudio_stanc("Dropbox/1_proj/via/me/code/stan/Via Model 1.1 - RW, Fixed LS, Sum Reduce.stan")
indus = read.csv("~/Dropbox/1_proj/logos/data/Industry Codes Updated B2B-C.csv")
head(indus)
indus = read.csv("~/Dropbox/1_proj/logos/data/Industry Codes Updated B2B-C.csv", row.names=1)
indus = read.csv("~/Dropbox/1_proj/logos/data/Industry Codes Updated B2B-C.csv")
indus = na.omit(indus)
head(indus)
dim(indus)
rownames(indus) = indus$X
head(indus)
indus = indus[,-1]
head(indus)
library(posterior)
nuuly_data <- read.csv("~/Dropbox/1_proj/urbn/data/nuuly_classified_embeddings.csv")
nuuly_data <- read.csv("~/Dropbox/1_proj/urbn/nikhil/nuuly_classified_embeddings.csv")
# Cleaning has already been done via Excel - use classified embeddings csv
library(tidyr)
library(Rtsne)
install.packages(tidyr)
install.packages(c("tidyr","Rtnse"))
# Running PCA on the Nuuly Data
counts <- c(10, 15, 20, 30)
i=10
nuuly_data_pca <- prcomp(nuuly_data[,10:73], center=TRUE, scale.=TRUE, rank.=i)
as.data.frame(nuuly_data_pca$x)
nuuly_data_pca$x
# Running PCA on the Nuuly Data
counts <- c(10, 15, 20, 30)
for (i in counts){
nuuly_data_pca <- prcomp(nuuly_data[,10:73], center=TRUE, scale.=TRUE, rank.=i)
## summary(nuuly_data_pca)
nuuly_output <- as.data.frame(nuuly_data_pca$x)
nuuly_output$Item_ID <- nuuly_data$Item.ID
nuuly_output <- nuuly_output[,c(i+1, 1:i)]
output_path <- paste("~/Dropbox/1_proj/urbn/nikhil/pca_out/nuuly_pca_general_",
toString(i), ".csv", sep="")
write.csv(nuuly_output, output_path, row.names = FALSE)
}
# Running PCA for all Classes for CSVs
counts <- c(10, 15, 20, 30)
classes <- c(1:8)
for (i in classes){
for (j in counts) {
nuuly_data_class <- nuuly_data %>% drop_na(colnames(nuuly_data)[i])
nuuly_data_pca <- prcomp(nuuly_data_class[,10:73], center=TRUE, scale.=TRUE,
rank.=j)
## summary(nuuly_data_pca)
nuuly_output <- as.data.frame(nuuly_data_pca$x)
nuuly_output$Item_ID <- nuuly_data_class$Item.ID
nuuly_output <- nuuly_output[,c(j+1, 1:j)]
output_path <- paste("~/Dropbox/1_proj/urbn/nikhil/pca_out/nuuly_pca_",
colnames(nuuly_data)[i], "_", toString(j), ".csv", sep="")
write.csv(nuuly_output, output_path, row.names = FALSE)
}
}
library("readxl")
nuuly_data <- read.csv("~/Dropbox/1_proj/urbn/nikhil/nuuly_classified_embeddings.csv")
# Cleaning has already been done via Excel - use classified embeddings csv
library(tidyr)
library(Rtsne)
# Running PCA for all Classes for CSVs
counts <- c(10, 15, 20, 30)
classes <- c(1:8)
for (i in classes){
for (j in counts) {
nuuly_data_class <- nuuly_data %>% drop_na(colnames(nuuly_data)[i])
nuuly_data_pca <- prcomp(nuuly_data_class[,10:73], center=TRUE, scale.=TRUE,
rank.=j)
## summary(nuuly_data_pca)
nuuly_output <- as.data.frame(nuuly_data_pca$x)
nuuly_output$Item_ID <- nuuly_data_class$Item.ID
nuuly_output <- nuuly_output[,c(j+1, 1:j)]
output_path <- paste("~/Dropbox/1_proj/urbn/nikhil/pca_out/nuuly_pca_",
colnames(nuuly_data)[i], "_", toString(j), ".csv", sep="")
write.csv(nuuly_output, output_path, row.names = FALSE)
}
}
install.packages("tidyverse")
library(tidyverse)
dresses10 = read.csv("~/Dropbox/1_proj/urbn/nikhil/pca_out/nuuly_pca_Dress_10.csv")
class(dresses10)
head(dresses10)
urls = read.csv("~/Dropbox/1_proj/urbn/data/nuuly_product_urls.csv")
head(urls)
urls[1,]
urls$image
urls %>% mutate(Item_ID = strsplit(Item_ID, split = "_")[1:2] %>% paste(collapse="_"))
urls %>% mutate(Item_ID = strsplit(image, split = "_")[1:2] %>% paste(collapse="_"))
image = urls$image[1]
image
strsplit(image, split = "_")
strsplit(image, split = "_")[[1]][1:2]
strsplit(image, split = "_")[[1]][1:2] %>% paste(collapse="_")
mutate(urls, Item_ID = strsplit(image, split = "_")[[1]][1:2] %>% paste(collapse="_"))
urls = mutate(urls, Item_ID = strsplit(image, split = "_")[[1]][1:2] %>% paste(collapse="_"))
head(urls)
urls = mutate(urls,
Item_ID = strsplit(image, split = "_")[[1]][1:2] %>% paste(collapse="_"),
Image_ID = strsplit(image, split = "_")[[1]][3])
urls$Image_ID
library(tidyverse)
dresses10 = read.csv("~/Dropbox/1_proj/urbn/nikhil/pca_out/nuuly_pca_Dress_10.csv")
urls = read.csv("~/Dropbox/1_proj/urbn/data/nuuly_product_urls.csv")
urls = mutate(urls,
Item_ID = strsplit(image, split = "_")[[1]][1:2] %>% paste(collapse="_"),
Image_ID = strsplit(image, split = "_")[[1]][3])
urls[1:5]
urls[1:5,]
strsplit(image$image, split = "_")[[1]][1:2] %>% paste(collapse="_")
strsplit(urls$image, split = "_")[[1]][1:2] %>% paste(collapse="_")
sapply(urls$image, function(x) strsplit(x, split = "_")[[1]][1:2] %>% paste(collapse="_")
Image_ID = strsplit(image, split = "_")[[1]][3])
sapply(urls$image, function(x) strsplit(x, split = "_")[[1]][1:2] %>% paste(collapse="_"))
urls$Item_ID = sapply(urls$image, function(x) strsplit(x, split = "_")[[1]][1:2] %>% paste(collapse="_"))
head(urls)
urls$Image_ID = sapply(urls$image, function(x) strsplit(x, split = "_")[[1]][-c(1:2)] %>% paste(collapse="_"))
head(urls)
urls = urls[order(urls$Item_ID, urls$Image_ID),]
head(urls)
urls_first = urls[!duplicated(urls$Item_ID),]
head(urls_first)
?join_left
library(tidyverse)
?left_join
test = left_join(dresses10, urls_first[,c("Item_ID","URL")], by = "Item_ID", )
head(test)
dresses10_urls = left_join(dresses10, urls_first[,c("Item_ID","URL")], by = "Item_ID", )
dresses10_urls = select(dresses10_urls, Item_ID, URL, everything())
head(dresses10_urls)
write.csv(dresses10_urls, file = "~/Dropbox/1_proj/urbn/data/dresses10_urls.csv")
write.csv(dresses10_urls, file = "~/Dropbox/1_proj/urbn/data/dresses10_urls.csv", row.names = FALSE)
27529/85729
27529^(-1) * 85729
1000/25
library(magrittr)
library(tidyr)
library(dplyr)
library(xtable)
base_dir = "~/Dropbox/1_proj/logos/code/pyro/"
setwd(base_dir)
all_files_dirs = list.files()
all_files_dirs = all_files_dirs[!(all_files_dirs %in% c("old", "test", "__pycache__"))]
dirs = all_files_dirs[sapply(strsplit(all_files_dirs, "[.]"), length) == 1]
id_cols = c("K","text_dec","bin_dec","cat_dec","bp_dec","indus_dec","all_enc","batches",
"iters","adam_lr","annealing_batches","num_particles","center_bp")
# process all available fit files:
for(dir in dirs) {
setwd(dir)
dir_files = list.files()
csvs = dir_files[sapply(strsplit(dir_files, "[.]"), function(x) length(x) > 1 & x[2] == "csv")]
for(csv in csvs){
cur_csv = select(read.csv(csv), -X)
run_num = strsplit(substr(csv, 1, nchar(csv)-4), split = "_")[[1]][1]
data_type = strsplit(substr(csv, 1, nchar(csv)-4), split = "_")[[1]][2]
feature_type = strsplit(substr(csv, 1, nchar(csv)-4), split = "_")[[1]][3]
if(data_type == "neighbors") {
next
}
if(feature_type == "bin") {
next
}
if(any(!(id_cols %in% colnames(cur_csv)))){
next
}
means = pivot_wider(cur_csv,
id_cols = id_cols,
values_from = setdiff(colnames(cur_csv), id_cols),
values_fn = mean) %>% as.data.frame()
means$dir = dir
means$run_num = run_num
means$data_type = data_type
if(feature_type == "metrics"){
if(data_type == "training") {
if(exists("training_table")) {
training_table = merge(training_table, means, all = TRUE)
} else {
training_table = means
}
} else {
if(exists("test_table")) {
test_table = merge(test_table, means, all = TRUE)
} else {
test_table = means
}
}
}
if(feature_type == "ll"){
if(data_type == "training") {
if(exists("training_ll")) {
training_ll = merge(training_ll, means, all = TRUE)
} else {
training_ll = means
}
} else {
if(exists("test_ll")) {
test_ll = merge(test_ll, means, all = TRUE)
} else {
test_ll = means
}
}
}
}
setwd("..")
}
select.metrics = function(tab, features, metrics, id_cols){
is_feat_met_col = sapply(strsplit(colnames(tab), "_"), function(x) x[1] %in% features & x[length(x)] %in% metrics)
is_id_col = colnames(tab) %in% id_cols
cols = c(which(is_id_col), which(is_feat_met_col))
tab[,cols]
}
id_cols = c("dir","run_num","data_type","K","text_dec","bin_dec","cat_dec","bp_dec","indus_dec","all_enc","batches",
"iters","adam_lr","annealing_batches","num_particles","center_bp")
logo_features = c("bin","cat1","cat2","cat3","cat4","cat5")
f1_auc_test_logo = select.metrics(test_table, features = logo_features, metrics = c("f1.score","auc"), id_cols)
f1_auc_test_logo[f1_auc_test_logo$data_type == "test",]
bp_test = select.metrics(test_table, features = "bp", metrics = c("mse","mae","mad"), id_cols)
bp_test[bp_test$data_type == "test",]
des_task = rbind(f1_auc_test_logo[f1_auc_test_logo$data_type == "des",],
f1_auc_test_logo[f1_auc_test_logo$dir == "NIR",])
des_task
mgr_task = rbind(bp_test[bp_test$data_type == "mgr",],
bp_test[bp_test$dir == "NIR",])
mgr_task
by(test_ll[test_ll$K == 20,]$bin_, INDICES = list(test_ll[test_ll$K == 20,]$num_particles, test_ll[test_ll$K == 20,]$dir), mean)
by(test_ll[test_ll$K == 20,]$bin_, INDICES = list(test_ll[test_ll$K == 20,]$num_particles, test_ll[test_ll$K == 20,]$dir), mean)
test_table %>%
filter(data_type == "test", num_particles < 20, center_bp == 0) %>%
select(run_num, dir, K, center_bp, bp_mse_)
des_task %>%
filter(num_particles < 20, center_bp == 0, dir %in% c("NIR", "Task-specific Inferences, Single Layer"))
test_ll %>%
filter(data_type == "logo", run_num == 1, num_particles == 1) %>%
arrange(bp_)
f1_auc_test_logo[f1_auc_test_logo$data_type == "test",]
test_ll %>%
filter(data_type == "logo", run_num == 1, num_particles == 1) %>%
arrange(bp_)
test_ll %>%
filter(data_type == "des", run_num == 1, num_particles == 1) %>%
arrange(bp_)
test_ll
training_ll
test_table
test_table %>%
filter(data_type == "test", num_particles < 20, center_bp == 1) %>%
select(run_num, dir, K, center_bp, bp_mse_)
test_table %>%
filter(data_type == "test", num_particles < 20, center_bp == 1) %>%
select(run_num, dir, K, center_bp, bp_mse_)
test_table %>%
filter(data_type == "test", num_particles < 20)#, center_bp == 1) %>%
test_table %>%
filter(data_type == "test", num_particles < 20, center_bp == 0) %>%
select(run_num, dir, K, center_bp, bp_mse_)
test_table %>%
filter(data_type == "test", num_particles < 20, center_bp == 0, K == 20) %>%
select(run_num, dir, K, center_bp, bp_mse_)
colnames(test_table)
test_table %>%
filter(data_type == "test", num_particles < 20, center_bp == 0, K == 20) %>%
select(run_num, dir, K, center_bp, bp_mse_, bin_micro_f1.score_, bin_macro_f1.score_)
bp_test = select.metrics(test_table, features = "bp", metrics = c("mse","mae","mad"), id_cols)
bp_test[bp_test$data_type == "test",]
bp_test = bp_test[bp_test$data_type == "test",]
bp_test[order(bp_test$bp_mse_),]
bp_test = select.metrics(training_table, features = "bp", metrics = c("mse","mae","mad"), id_cols)
bp_test = bp_test[bp_test$data_type == "test",]
bp_test[order(bp_test$bp_mse_),]
bp_test = bp_test[bp_test$data_type == "training",]
bp_test = select.metrics(training_table, features = "bp", metrics = c("mse","mae","mad"), id_cols)
bp_test = bp_test[bp_test$data_type == "training",]
bp_test[order(bp_test$bp_mse_),]
bp_test = select.metrics(test_table, features = "bp", metrics = c("mse","mae","mad"), id_cols)
bp_test = bp_test[bp_test$data_type == "test",]
bp_test[order(bp_test$bp_mse_),]
bp_test %>% filter(data_type == "test") %>% arrange(bp_mse_)
test_ll %>%
filter(data_type == "test", num_particles == 1) %>%
arrange(bp_)
debugSource('~/Dropbox/1_proj/logos/code/pyro/metrics_tables.R', echo=TRUE)
test_ll %>%
filter(data_type == "test", num_particles == 1) %>%
arrange(bin_)
test_ll %>%
filter(data_type == "test", num_particles == 1) %>%
arrange(cat1_)
test_table %>%
filter(data_type == "test", num_particles == 1, center_bp == 0) %>%
select(bin_micro_f1.score_, bin_macro_f1.score_, cat1_macro_f1.score_, cat1_micro_f1.score_, text_auc_, indus_micro_f1.score_, indus_macro_f1.score_, bp_mse_)
test_table %>%
filter(data_type == "test", num_particles == 1, center_bp == 0) %>%
select(dir, K, text_dec, enc, bin_micro_f1.score_, bin_macro_f1.score_, cat1_macro_f1.score_, cat1_micro_f1.score_, text_auc_, indus_micro_f1.score_, indus_macro_f1.score_, bp_mse_)
columns(test_table)
colnames(test_table)
test_table %>%
filter(data_type == "test", num_particles == 1, center_bp == 0) %>%
select(dir, K, text_dec, all_enc, bin_micro_f1.score_, bin_macro_f1.score_, cat1_macro_f1.score_, cat1_micro_f1.score_, text_auc_, indus_micro_f1.score_, indus_macro_f1.score_, bp_mse_)
test_table %>%
filter(data_type == "test", num_particles == 1, center_bp == 0) %>%
select(dir, K, text_dec, all_enc, bin_micro_f1.score_, bin_macro_f1.score_, cat1_macro_f1.score_, cat1_micro_f1.score_,
text_auc_, indus_micro_f1.score_, indus_macro_f1.score_, bp_mse_) %>%
arrange(text_auc_)
test_table %>%
filter(data_type == "test", num_particles == 1, center_bp == 0) %>%
select(dir, K, text_dec, all_enc, bin_micro_f1.score_, bin_macro_f1.score_, cat1_macro_f1.score_, cat1_micro_f1.score_,
text_auc_, indus_micro_f1.score_, indus_macro_f1.score_, bp_mse_) %>%
arrange(indus_macro_f1.score_)
test_table %>%
filter(data_type == "test", num_particles == 1, center_bp == 0) %>%
select(dir, K, text_dec, all_enc, bin_micro_f1.score_, bin_macro_f1.score_, cat1_macro_f1.score_, cat1_micro_f1.score_,
text_auc_, indus_micro_f1.score_, indus_macro_f1.score_, bp_mse_) %>%
arrange(indus_micro_f1.score_)
test_table %>%
filter(data_type == "test", num_particles == 1, center_bp == 0, dir != "Just Designer") %>%
select(dir, K, text_dec, all_enc, bin_micro_f1.score_, bin_macro_f1.score_, cat1_macro_f1.score_, cat1_micro_f1.score_,
text_auc_, indus_micro_f1.score_, indus_macro_f1.score_, bp_mse_) %>%
arrange(indus_micro_f1.score_)
?arrange
test_table %>%
filter(data_type == "test", num_particles == 1, center_bp == 0, dir != "Just Designer") %>%
select(dir, K, text_dec, all_enc, bin_micro_f1.score_, bin_macro_f1.score_, cat1_macro_f1.score_, cat1_micro_f1.score_,
text_auc_, indus_micro_f1.score_, indus_macro_f1.score_, bp_mse_) %>%
arrange(desc(indus_micro_f1.score_))
test_stats_filtered = test_table %>%
filter(data_type == "test", num_particles == 1, center_bp == 0, dir != "Just Designer") %>%
select(dir, K, text_dec, all_enc, bin_micro_f1.score_, bin_macro_f1.score_, cat1_macro_f1.score_, cat1_micro_f1.score_,
text_auc_, indus_micro_f1.score_, indus_macro_f1.score_, bp_mse_)
test_stats_filtered %>%
arrange(desc(indus_micro_f1.score_))
test_stats_filtered %>%
arrange(desc(bin_micro_f1.score_))
test_stats_filtered %>%
arrange(desc(bin_macro_f1.score_))
test_stats_filtered %>%
arrange(desc(cat1_macro_f1.score_))
test_stats_filtered %>%
arrange(desc(cat1_micro_f1.score_))
test_stats_filtered %>%
arrange(desc(text_auc_))
test_stats_filtered %>%
arrange(desc(bp_mse_))
bp_test %>% filter(data_type == "test") %>% arrange(bp_mse_)
