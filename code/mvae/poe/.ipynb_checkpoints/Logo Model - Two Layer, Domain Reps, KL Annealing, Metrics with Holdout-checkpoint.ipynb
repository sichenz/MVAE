{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.examples.util  # patches torchvision\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro import poutine\n",
    "\n",
    "pyro.set_rng_seed(42)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from skimage import io\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pyro.__version__.startswith('1.3.0')\n",
    "pyro.enable_validation(True)\n",
    "pyro.distributions.enable_validation(False)\n",
    "\n",
    "# Enable smoke test - run the notebook cells on CI.\n",
    "smoke_test = 'CI' in os.environ\n",
    "\n",
    "# torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_onehot(intvec):\n",
    "    N = len(intvec)\n",
    "    K = max(intvec)\n",
    "    \n",
    "    return np.array([np.array([1 if i==intvec[j] else 0 for i in range(K+1)]) for j in range(N)])\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## THIS IS A DIRECT COPY FROM THE EDWARD CODE, FOR REFERENCE:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ndomains = 4\n",
    "batch_size = 500\n",
    "domain_samples = int(batch_size/ndomains)\n",
    "\n",
    "# How many iterations to use when testing DGP:\n",
    "# NOTE: THIS IS CONFUSING; IT'S JUST FOR DOING MONTE CARLO ESTIMATES\n",
    "# OF EXPECTATIONS DURING THE LAST PART OF THE PROGRAM, IT'S NOT THE\n",
    "# # OF ITERS FOR THE OPTIMIZATION\n",
    "sim_iters = 1000\n",
    "\n",
    "# How many cross-validation folds should there be?\n",
    "nfolds = 4\n",
    "\n",
    "# Optimization parameters:\n",
    "batches = 1000\n",
    "niter_per_batch = 100\n",
    "samps = 1\n",
    "\n",
    "# Inference network size:\n",
    "j = 1024\n",
    "\n",
    "# Latent space size:\n",
    "k = 40\n",
    "\n",
    "# Decoder sizes:\n",
    "text = 1024\n",
    "logo = 512\n",
    "indus = 512\n",
    "bp = 512\n",
    "\n",
    "# Regularization parameters:\n",
    "kl_scale = 1.0\n",
    "r = 10.0\n",
    "\n",
    "\n",
    "# This list format is outdated, just carried over from grid search code:\n",
    "sim_info = [\n",
    "    {\"name\": \"final\",\n",
    "     \"K\": k,\n",
    "     \"L1_logos\": logo, \"L2_logos\": 0, \"L1_text\": text, \"L2_text\": 0, \"L1_bp\": bp, \"L2_bp\": 0,\n",
    "     \"L1_indus\": indus, \"L2_indus\": 0,\n",
    "     \"logo_enc_L1\": j, \"logo_enc_L2\": 0, \"nl_enc_L1\": j, \"nl_enc_L2\": 0,\n",
    "     \"nbp_enc_L1\": j, \"nbp_enc_L2\": 0, \"full_enc_L1\": j, \"full_enc_L2\": 0,\n",
    "     \"regu_lambda\": [r, r], \"enc_regu\": 0.0, \"kl_scale\": kl_scale,\n",
    "     \"learning_rate\": 0.0001, \"n_samples\": samps, \"nbatches\": batches, \"niter_per_batch\": niter_per_batch,\n",
    "     \"drop_rate\": 0.5, \"enc_drop_rate\": 0.5\n",
    "    }\n",
    "]\n",
    "\n",
    "sim_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "First, load text data, and apply word filter. Note on notation: `tx` stands for \"true x,\" because the model variables are also called x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf = pd.read_csv(\"../../data/web_dtfm20_binary.csv\", index_col=0)\n",
    "tx_text = textdf.values\n",
    "seltext = tx_text.sum(0) > 0.05\n",
    "tx_text = textdf.values[:,seltext]\n",
    "\n",
    "gt20words = tx_text.sum(1) > 20\n",
    "tx_text = tx_text[gt20words,:]\n",
    "\n",
    "words = textdf.columns[seltext]\n",
    "N, V = tx_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarized logo features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "binfeats = pd.read_csv(\"../../data/y_bin_all_py2.csv\", index_col=0)\n",
    "tx_b = binfeats.values\n",
    "tx_b = tx_b[gt20words,:]\n",
    "M_b = tx_b.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WE DON'T NEED THESE AS ONE-HOT ENCODED ANYMORE, NEED TO REDO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "catfeats = pd.read_csv(\"../../data/y_mult_ncolors_py2.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_c1 = catfeats.values[:,0][gt20words]\n",
    "M_c1 = len(np.unique(tx_c1))\n",
    "tx_c1 = np.expand_dims(tx_c1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_c2 = catfeats.values[:,1][gt20words]\n",
    "M_c2 = len(np.unique(tx_c2))\n",
    "tx_c2 = np.expand_dims(tx_c2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_c3 = catfeats.values[:,2][gt20words]\n",
    "M_c3 = len(np.unique(tx_c3))\n",
    "tx_c3 = np.expand_dims(tx_c3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_c4 = catfeats.values[:,3][gt20words]\n",
    "M_c4 = len(np.unique(tx_c4))\n",
    "tx_c4 = np.expand_dims(tx_c4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_c5 = catfeats.values[:,4][gt20words]\n",
    "M_c5 = len(np.unique(tx_c5))\n",
    "tx_c5 = np.expand_dims(tx_c5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 6, 16, 3, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_c1, M_c2, M_c3, M_c4, M_c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_labels = np.array([\"black\",\"blue_dark\",\"blue_light\",\"blue_medium\",\"brown\",\"green_dark\",\n",
    "                      \"green_light\",\"grey_dark\",\"grey_light\",\"orange\",\"red\",\"red_dark\",\n",
    "                      \"yellow\"])\n",
    "\n",
    "c2_labels = np.array([\"circle\",\"rect-oval_medium\",\"rect-oval_large\",\"rect-oval_thin\",\n",
    "                      \"square\",\"triangle\"])\n",
    "\n",
    "c3_labels = np.array([\"bad_letters\",\"bulky_hollow_geometric\",\"circular\",\"dense_simple_geom\",\n",
    "                      \"detailed_circle\",\"hollow_circle\",\"detailed_hor\",\"long_hor\",\"no_mark\",\n",
    "                      \"simple\",\"square\",\"thin_vert_rect\",\"vert_narrow\",\"detailed\",\"thin\",\n",
    "                      \"hor_wispy\"])\n",
    "\n",
    "c4_labels = np.array([\"nochars\",\"sans\",\"serif\"])\n",
    "\n",
    "c5_labels = np.array([\"one_color\",\"two_colors\",\"three_colors\",\"many_colors\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load brand personality ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = pd.read_csv(\"../../data/bp_avg_all_traits.csv\", index_col=0)\n",
    "\n",
    "tx_bp = bp.values\n",
    "tx_bp = tx_bp[gt20words]\n",
    "# tx_bp = (tx_bp - tx_bp.mean(0)) / tx_bp.std(0)\n",
    "M_bp = tx_bp.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load industry tags, filtering by those that show up at least 10 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "indus = pd.read_csv(\"../../data/industry_codes_updated.csv\", index_col=0)\n",
    "indus = indus.iloc[np.in1d(indus.index, bp.index),:]\n",
    "indus = indus.sort_index()\n",
    "\n",
    "tx_indus = indus.values.astype('int')\n",
    "tx_indus = tx_indus[:, tx_indus.sum(0) > 9]\n",
    "tx_indus = tx_indus[gt20words,:]\n",
    "M_indus = tx_indus.shape[1]\n",
    "\n",
    "indus_labels = indus.columns[indus.values.sum(0) > 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save names of firms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "allnames = binfeats.index.values[gt20words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create holdout and cross-validation subsets, then \"generate\" the indices for a given fold:\n",
    "\n",
    "*FOR NOW, I WILL COMMENT OUT ALL OF THESE. THEY REQUIRE US TO BE DOING K-FOLD CV, BUT FOR STARTING, I WILL USE THE FULL DATA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split(a, n):\n",
    "#     k, m = divmod(len(a), n)\n",
    "#     return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "\n",
    "# holdout_indices = list(split(np.arange(N), nfolds))\n",
    "# holdout_indices.append([])\n",
    "# fold_indices = [np.setdiff1d(np.arange(N), holdout_indices[i]) for i in range(nfolds)]\n",
    "# fold_indices.append(np.arange(N))\n",
    "\n",
    "# def generator(fold):\n",
    "#     indices = np.random.choice(fold_indices[fold], replace=False, size=batch_size)\n",
    "#     return tx_b[indices], tx_c1[indices], tx_c2[indices], tx_c3[indices], tx_c4[indices], tx_c5[indices], tx_text[indices], tx_bp[indices], tx_indus[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the batch among the different inference networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_indices_split(n_domains, batch_size):\n",
    "    index_list = []\n",
    "    for d in range(n_domains):\n",
    "        index_list.append(np.random.choice(np.setdiff1d(np.arange(0, batch_size), index_list),\n",
    "                                  replace = False, size = int(batch_size/n_domains)).astype('int'))\n",
    "    return index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "First, I'll define some generic encoder/decoder functions, that can be used across all model components. These are modified from the Pyro VAE tutorial. \n",
    "\n",
    "**Note: in the VAE tutorial, they use softplus activations. We used ReLU. I wonder if it makes a difference?**\n",
    "\n",
    "Most sources I can find suggest RELU or ELU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalDecoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, x_size):\n",
    "        super().__init__()\n",
    "        # set up the two linear transformations used\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim + z_dim, x_size)\n",
    "        # set up the non-linearities\n",
    "        self.activation = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # set up dropout\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, z):\n",
    "        # define the forward computation on the latent z\n",
    "        # first compute the hidden units\n",
    "        hidden = self.dropout(self.activation(self.fc1(z)))\n",
    "        # return the parameter for the output categorical (i.e., softmax probs)\n",
    "        x_probs = self.softmax(self.fc21(torch.cat((hidden, z), 1)))\n",
    "        return x_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDecoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim, x_size):\n",
    "        super().__init__()\n",
    "        # set up the two linear transformations used\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim + z_dim, x_size)\n",
    "        self.fc22 = nn.Linear(hidden_dim + z_dim, x_size)\n",
    "        # set up the non-linearities\n",
    "        self.activation = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        # set up dropout\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, z):\n",
    "        # define the forward computation on the latent z\n",
    "        # first compute the hidden units\n",
    "        hidden = self.dropout(self.activation(self.fc1(z)))\n",
    "        # return the parameter for the output reals\n",
    "        x_loc = self.fc21(torch.cat((hidden, z), 1))\n",
    "        x_scale = self.softplus(self.fc22(torch.cat((hidden, z), 1)))\n",
    "        return x_loc, x_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimates the z vectors for new data:\n",
    "class NewZ():\n",
    "    def __init__(self, lmvae, x_text, x_bin, x_cat1, x_cat2, x_cat3, x_cat4, x_cat5, x_bp, x_indus, samples=100):\n",
    "        with torch.no_grad():\n",
    "            N = x_text.shape[0]\n",
    "\n",
    "            x_logo = torch.cat((x_bin.float(), x_cat1.float(), \n",
    "                                x_cat2.float(), x_cat3.float(), x_cat4.float(), \n",
    "                                x_cat5.float()), 1)\n",
    "\n",
    "            self.z_text_loc, self.z_text_scale = lmvae.text_encoder.forward(x_text)\n",
    "            z_text = dist.Normal(self.z_text_loc, self.z_text_scale).sample(torch.Size([samples]))\n",
    "\n",
    "            self.z_logo_loc, self.z_logo_scale = lmvae.logo_encoder.forward(x_logo)\n",
    "            z_logo = dist.Normal(self.z_logo_loc, self.z_logo_scale).sample(torch.Size([samples]))\n",
    "\n",
    "            self.z_bp_loc, self.z_bp_scale = lmvae.bp_encoder.forward(x_bp)\n",
    "            z_bp = dist.Normal(self.z_bp_loc, self.z_bp_scale).sample(torch.Size([samples]))\n",
    "\n",
    "            self.z_indus_loc, self.z_indus_scale = lmvae.indus_encoder.forward(x_indus)\n",
    "            z_indus = dist.Normal(self.z_indus_loc, self.z_indus_scale).sample(torch.Size([samples]))\n",
    "\n",
    "            z0_loc, z0_scale = lmvae.upper_encoder.forward(z_text, z_logo, z_bp, z_indus)\n",
    "            self.z0_loc = z0_loc.mean(0)\n",
    "            self.z0_scale = z0_scale.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given z, predicts the average feature values (or feature probabilities), and saves them:\n",
    "class Predict():\n",
    "    def __init__(self, lmvae, z, x_text, x_bin, x_cat1, x_cat2, x_cat3, x_cat4, x_cat5, x_bp, x_indus, samples = 100):\n",
    "        with torch.no_grad():    \n",
    "            self.text = lmvae.text_decoder(dist.Normal(z.z_text_loc, z.z_text_scale).sample()).detach().cpu().numpy()/samples\n",
    "            self.bin = lmvae.bin_decoder(dist.Normal(z.z_logo_loc, z.z_logo_scale).sample()).detach().cpu().numpy()/samples\n",
    "            self.cat1 = lmvae.cat1_decoder(dist.Normal(z.z_logo_loc, z.z_logo_scale).sample()).detach().cpu().numpy()/samples\n",
    "            self.cat2 = lmvae.cat2_decoder(dist.Normal(z.z_logo_loc, z.z_logo_scale).sample()).detach().cpu().numpy()/samples\n",
    "            self.cat3 = lmvae.cat3_decoder(dist.Normal(z.z_logo_loc, z.z_logo_scale).sample()).detach().cpu().numpy()/samples\n",
    "            self.cat4 = lmvae.cat4_decoder(dist.Normal(z.z_logo_loc, z.z_logo_scale).sample()).detach().cpu().numpy()/samples\n",
    "            self.cat5 = lmvae.cat5_decoder(dist.Normal(z.z_logo_loc, z.z_logo_scale).sample()).detach().cpu().numpy()/samples\n",
    "            self.bp = lmvae.bp_decoder(dist.Normal(z.z_bp_loc, z.z_bp_scale).sample())[0].detach().cpu().numpy()/samples\n",
    "            self.indus = lmvae.indus_decoder(dist.Normal(z.z_indus_loc, z.z_indus_scale).sample()).detach().cpu().numpy()/samples\n",
    "\n",
    "            for _ in range(samples-1):\n",
    "                self.text += lmvae.text_decoder(dist.Normal(z.z_text_loc, z.z_text_scale).sample()).detach().cpu().numpy()/samples\n",
    "                self.bin += lmvae.bin_decoder(dist.Normal(z.z_logo_loc, z.z_logo_scale).sample()).detach().cpu().numpy()/samples\n",
    "                self.cat1 += lmvae.cat1_decoder(dist.Normal(z.z_logo_loc, z.z_logo_scale).sample()).detach().cpu().numpy()/samples\n",
    "                self.cat2 += lmvae.cat2_decoder(dist.Normal(z.z_logo_loc, z.z_logo_scale).sample()).detach().cpu().numpy()/samples\n",
    "                self.cat3 += lmvae.cat3_decoder(dist.Normal(z.z_logo_loc, z.z_logo_scale).sample()).detach().cpu().numpy()/samples\n",
    "                self.cat4 += lmvae.cat4_decoder(dist.Normal(z.z_logo_loc, z.z_logo_scale).sample()).detach().cpu().numpy()/samples\n",
    "                self.cat5 += lmvae.cat5_decoder(dist.Normal(z.z_logo_loc, z.z_logo_scale).sample()).detach().cpu().numpy()/samples\n",
    "                self.bp += lmvae.bp_decoder(dist.Normal(z.z_bp_loc, z.z_bp_scale).sample())[0].detach().cpu().numpy()/samples\n",
    "                self.indus += lmvae.indus_decoder(dist.Normal(z.z_indus_loc, z.z_indus_scale).sample()).detach().cpu().numpy()/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a set of predictions of class Predict(), computes fit statistics:\n",
    "class Metrics():\n",
    "    def __init__(self, pred, x_text, x_bin, x_cat1, x_cat2, x_cat3, x_cat4, x_cat5, x_bp, x_indus):\n",
    "        with torch.no_grad():\n",
    "            self.mae_text = MAE(pred.text, x_text.detach().cpu().numpy())\n",
    "            self.mae_bin = MAE(pred.bin, x_bin.detach().cpu().numpy())\n",
    "\n",
    "            self.report_text = classification_report(x_text.detach().cpu().numpy(), pred.text > 0.5)\n",
    "            self.report_bin = classification_report(x_bin.detach().cpu().numpy(), pred.bin > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for new data, compute the z vectors, then use that to make predictions and compute metrics\n",
    "class LMVAE_Test():\n",
    "    def __init__(self, lmvae, x_text, x_bin, x_cat1, x_cat2, x_cat3, x_cat4, x_cat5, x_bp, x_indus):\n",
    "        self.N = x_text.shape[0]\n",
    "        self.z = NewZ(lmvae, x_text, x_bin, x_cat1, x_cat2, x_cat3, x_cat4, x_cat5, x_bp, x_indus, samples=100)      \n",
    "        self.values = Predict(lmvae, self.z, x_text, x_bin, x_cat1, x_cat2, x_cat3, x_cat4, x_cat5, x_bp, x_indus)\n",
    "        self.metrics = Metrics(self.values, x_text, x_bin, x_cat1, x_cat2, x_cat3, x_cat4, x_cat5, x_bp, x_indus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's put these components together into a MVAE class that combines all of these components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dims = {\"upper\": 200, \"text\": 400, \"bin\": 400, \"cat\": 400, \"bp\": 400, \"indus\": 400}\n",
    "x_sizes = {\"text\": V, \"bin\": M_b, \"cat1\": M_c1, \"cat2\": M_c2, \"cat3\": M_c3, \"cat4\": M_c4, \n",
    "          \"cat5\": M_c5, \"bp\": M_bp, \"indus\": M_indus, \"logo\": M_b + 5, \"all\": V + M_b + 5 + M_bp + M_indus}\n",
    "encoder_dims = [200, 1000]\n",
    "task_sizes = {\"full\": x_sizes[\"all\"], \n",
    "              \"logo\": x_sizes[\"bin\"] + 5, \n",
    "              \"design\": x_sizes[\"text\"] + x_sizes[\"bp\"] + x_sizes[\"indus\"], \n",
    "              \"mgr\": x_sizes[\"all\"] - x_sizes[\"bp\"]}\n",
    "K = [20, 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "lmvae = LogoMVAE(K[0], K[1], encoder_dims, decoder_dims, x_sizes, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam({\"lr\": 0.0001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi = SVI(lmvae.model, lmvae.guide, optimizer, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 2\n",
    "n_iter = 1\n",
    "\n",
    "min_af = 0.1\n",
    "annealing_batches = 0\n",
    "# kl_annealing_schedule = np.linspace(min_af, 1., annealing_batches)\n",
    "\n",
    "#  This is for cyclical annealing, didn't seem to work that well:\n",
    "kl_annealing_schedule = np.concatenate([np.linspace(min_af, 1., round(annealing_batches/4.)) for _ in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 11.76it/s]\n"
     ]
    }
   ],
   "source": [
    "track_loss = []\n",
    "for i in tqdm(range(n_batches)):\n",
    "    \n",
    "    if i < annealing_batches:\n",
    "        annealing_factor = kl_annealing_schedule[i]\n",
    "    else:\n",
    "        annealing_factor = 1.\n",
    "        \n",
    "    rand_order = np.random.choice(np.arange(N), replace=False, size=N)\n",
    "    \n",
    "    x_text = torch.tensor(tx_text[rand_order], dtype = torch.float).cuda()\n",
    "    x_bin = torch.tensor(tx_b[rand_order], dtype = torch.float).cuda()\n",
    "    x_cat1 = torch.tensor(tx_c1[rand_order], dtype = torch.float).cuda()\n",
    "    x_cat2 = torch.tensor(tx_c2[rand_order], dtype = torch.float).cuda()\n",
    "    x_cat3 = torch.tensor(tx_c3[rand_order], dtype = torch.float).cuda()\n",
    "    x_cat4 = torch.tensor(tx_c4[rand_order], dtype = torch.float).cuda()\n",
    "    x_cat5 = torch.tensor(tx_c5[rand_order], dtype = torch.float).cuda()\n",
    "    x_bp = torch.tensor(tx_bp[rand_order], dtype = torch.float).cuda()\n",
    "    x_indus = torch.tensor(tx_indus[rand_order], dtype = torch.float).cuda()\n",
    "    \n",
    "    for j in range(n_iter):\n",
    "        svi.step(x_text, x_bin, x_cat1, x_cat2, x_cat3, x_cat4, x_cat5, x_bp, x_indus, annealing_factor)\n",
    "        track_loss.append(svi.evaluate_loss(x_text, x_bin, x_cat1, x_cat2, x_cat3, x_cat4, x_cat5, x_bp, x_indus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f26fcf1fa50>]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(track_loss[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro",
   "language": "python",
   "name": "pyro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
