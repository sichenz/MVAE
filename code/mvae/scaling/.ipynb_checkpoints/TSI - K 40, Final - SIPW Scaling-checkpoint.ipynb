{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_index = 'k40_final_scaling'\n",
    "\n",
    "DECODER_DIMS = {\"text\": 400, \"bin\": 400, \"cat\": 400, \"bp\": 400, \"indus\": 400}\n",
    "ENCODER_DIMS = {\"full\": 1000, \"logo\": 100, \"mgr\": 1000, \"design\": 1000}\n",
    "K = 40\n",
    "\n",
    "FOLDS = 4\n",
    "BATCHES = 2500\n",
    "ITERS = 10\n",
    "\n",
    "ADAM_LR = 0.00001\n",
    "MIN_AF = 1e-6\n",
    "ANNEALING_BATCHES = 2000\n",
    "NUM_PARTICLES = 1\n",
    "\n",
    "CENTER_BP = True\n",
    "\n",
    "DISABLE_TQDM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../../')\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.examples.util  # patches torchvision\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro import poutine\n",
    "\n",
    "pyro.set_rng_seed(42)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from skimage import io\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from data import SplitData\n",
    "from model import LogoMVAE\n",
    "\n",
    "assert pyro.__version__.startswith('1.3.0')\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "\n",
    "def compute_distance(z):\n",
    "    b = z.reshape(z.shape[0], 1, z.shape[1])\n",
    "    return np.sqrt(np.einsum('ijk, ijk->ij', z-b, z-b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "First, load text data, and apply word filter. Note on notation: `tx` stands for \"true x,\" because the model variables are also called x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf = pd.read_csv(\"../../../../data/web_dtfm20_binary.csv\", index_col=0)\n",
    "tx_text = textdf.values\n",
    "seltext = tx_text.sum(0) > 0.05\n",
    "tx_text = textdf.values[:,seltext]\n",
    "\n",
    "gt20words = tx_text.sum(1) > 20\n",
    "tx_text = tx_text[gt20words,:]\n",
    "\n",
    "words = textdf.columns[seltext]\n",
    "N, V = tx_text.shape\n",
    "\n",
    "binfeats = pd.read_csv(\"../../../../data/y_bin_all_py2.csv\", index_col=0)\n",
    "tx_b = binfeats.values\n",
    "tx_b = tx_b[gt20words,:]\n",
    "M_b = tx_b.shape[1]\n",
    "\n",
    "catfeats = pd.read_csv(\"../../../../data/y_mult_ncolors_py2.csv\", index_col=0)\n",
    "\n",
    "tx_c1 = catfeats.values[:,0][gt20words]\n",
    "M_c1 = len(np.unique(tx_c1))\n",
    "tx_c1 = np.expand_dims(tx_c1, 1)\n",
    "\n",
    "tx_c2 = catfeats.values[:,1][gt20words]\n",
    "M_c2 = len(np.unique(tx_c2))\n",
    "tx_c2 = np.expand_dims(tx_c2, 1)\n",
    "\n",
    "tx_c3 = catfeats.values[:,2][gt20words]\n",
    "M_c3 = len(np.unique(tx_c3))\n",
    "tx_c3 = np.expand_dims(tx_c3, 1)\n",
    "\n",
    "tx_c4 = catfeats.values[:,3][gt20words]\n",
    "M_c4 = len(np.unique(tx_c4))\n",
    "tx_c4 = np.expand_dims(tx_c4, 1)\n",
    "\n",
    "tx_c5 = catfeats.values[:,4][gt20words]\n",
    "M_c5 = len(np.unique(tx_c5))\n",
    "tx_c5 = np.expand_dims(tx_c5, 1)\n",
    "\n",
    "c1_labels = np.array([\"black\",\"blue_dark\",\"blue_light\",\"blue_medium\",\"brown\",\"green_dark\",\n",
    "                      \"green_light\",\"grey_dark\",\"grey_light\",\"orange\",\"red\",\"red_dark\",\n",
    "                      \"yellow\"])\n",
    "\n",
    "c2_labels = np.array([\"circle\",\"rect-oval_medium\",\"rect-oval_large\",\"rect-oval_thin\",\n",
    "                      \"square\",\"triangle\"])\n",
    "\n",
    "c3_labels = np.array([\"bad_letters\",\"bulky_hollow_geometric\",\"circular\",\"dense_simple_geom\",\n",
    "                      \"detailed_circle\",\"hollow_circle\",\"detailed_hor\",\"long_hor\",\"no_mark\",\n",
    "                      \"simple\",\"square\",\"thin_vert_rect\",\"vert_narrow\",\"detailed\",\"thin\",\n",
    "                      \"hor_wispy\"])\n",
    "\n",
    "c4_labels = np.array([\"nochars\",\"sans\",\"serif\"])\n",
    "\n",
    "c5_labels = np.array([\"one_color\",\"two_colors\",\"three_colors\",\"many_colors\"])\n",
    "\n",
    "bp = pd.read_csv(\"../../../../data/bp_avg_all_traits.csv\", index_col=0)\n",
    "\n",
    "bp_labels = bp.columns\n",
    "\n",
    "tx_bp = bp.values\n",
    "tx_bp = tx_bp[gt20words]\n",
    "if CENTER_BP:\n",
    "    tx_bp = (tx_bp - tx_bp.mean(0)) / tx_bp.std(0)\n",
    "M_bp = tx_bp.shape[1]\n",
    "\n",
    "indus = pd.read_csv(\"../../../../data/industry_codes_updated.csv\", index_col=0)\n",
    "indus = indus.iloc[np.in1d(indus.index, bp.index),:]\n",
    "indus = indus.sort_index()\n",
    "\n",
    "tx_indus = indus.values.astype('int')\n",
    "tx_indus = tx_indus[:, tx_indus.sum(0) > 9]\n",
    "tx_indus = tx_indus[gt20words,:]\n",
    "M_indus = tx_indus.shape[1]\n",
    "\n",
    "indus_labels = indus.columns[indus.values.sum(0) > 9]\n",
    "\n",
    "allnames = binfeats.index.values[gt20words]\n",
    "\n",
    "x_sizes = {\"text\": V, \n",
    "           \"bin\": M_b, \n",
    "           \"cat1\": M_c1, \n",
    "           \"cat2\": M_c2, \n",
    "           \"cat3\": M_c3, \n",
    "           \"cat4\": M_c4, \n",
    "           \"cat5\": M_c5, \n",
    "           \"bp\": M_bp, \n",
    "           \"indus\": M_indus, \n",
    "           \"logo\": M_b + 5, \n",
    "           \"all\": V + M_b + 5 + M_bp + M_indus}\n",
    "\n",
    "task_sizes = {\"full\": x_sizes[\"all\"], \n",
    "              \"logo\": x_sizes[\"bin\"] + 5, \n",
    "              \"design\": x_sizes[\"text\"] + x_sizes[\"bp\"] + x_sizes[\"indus\"], \n",
    "              \"mgr\": x_sizes[\"all\"] - x_sizes[\"bp\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: Instantiate Model and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "givens = pd.DataFrame(np.concatenate(([[K], list(DECODER_DIMS.values()), list(ENCODER_DIMS.values()), [BATCHES], [ITERS], [ADAM_LR], [ANNEALING_BATCHES], [NUM_PARTICLES], [CENTER_BP]]))).T\n",
    "givens.columns = [\"K\", \"text_dec\", \"bin_dec\", \"cat_dec\", \"bp_dec\", \"indus_dec\", \"full_enc\", \"logo_enc\", \"mgr_enc\", \"design_enc\", \"batches\", \"iters\", \"adam_lr\", \"annealing_batches\", \"num_particles\", \"center_bp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create holdout and cross-validation subsets (just the indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FOLDS > 1:\n",
    "    holdout_indices = list(split(np.arange(N), FOLDS))\n",
    "    holdout_indices.append(np.array([]))\n",
    "    fold_indices = [np.setdiff1d(np.arange(N), holdout_indices[i]) for i in range(FOLDS)]\n",
    "    fold_indices.append(np.arange(N))\n",
    "else:\n",
    "    holdout_indices = [np.array([])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the KL annealing schedule (same across each fold):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = np.linspace(MIN_AF, 1., ANNEALING_BATCHES)\n",
    "# schedule = np.concatenate([np.linspace(MIN_AF, 1., round(ANNEALING_BATCHES/4.)) for _ in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_logo_bp_mse = []\n",
    "track_logo_indus_macf1 = []\n",
    "track_mgr_bp_mse = []\n",
    "track_des_bin_macf1 = []\n",
    "track_des_cat1_macf1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9352b5a7545a40dbae961d414098ad1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Folds', max=5.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be09df9664444775b7df9e0dd3804f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=2500.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the model across all folds (sequentially):\n",
    "for fold in tqdm(range(FOLDS+1), desc=\"Folds\", disable=DISABLE_TQDM):\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    data = SplitData(tx_text, tx_b, tx_c1, tx_c2, tx_c3, tx_c4, tx_c5, tx_bp, tx_indus, \n",
    "                     allnames, test_indices = holdout_indices[fold])   \n",
    "\n",
    "    lmvae = LogoMVAE(K, ENCODER_DIMS, DECODER_DIMS, x_sizes, task_sizes, use_cuda=True)\n",
    "    optimizer = Adam({\"lr\": ADAM_LR}) #, \"weight_decay\": 0.4})\n",
    "    svi = SVI(lmvae.model, lmvae.guide, optimizer, loss=Trace_ELBO(num_particles = NUM_PARTICLES))\n",
    "\n",
    "    track_loss = []\n",
    "\n",
    "    for i in tqdm(range(BATCHES), desc=\"Batches\", leave=False, disable=DISABLE_TQDM):\n",
    "\n",
    "        if i < ANNEALING_BATCHES:\n",
    "            annealing_factor = schedule[i]\n",
    "        else:\n",
    "            annealing_factor = 1.\n",
    "\n",
    "        data.training.shuffle()\n",
    "\n",
    "        for j in tqdm(range(ITERS), desc=\"Iters\", leave=False, disable=True):\n",
    "            svi.step(data.training, annealing_factor)\n",
    "            track_loss.append(svi.evaluate_loss(data.training, annealing_factor))\n",
    "        \n",
    "        if (i % 20) == 0:\n",
    "            if hasattr(data, 'test'):\n",
    "\n",
    "                data.test.make_torch()\n",
    "\n",
    "                lmvae.eval()\n",
    "\n",
    "                lmvae.predict(data.test, network = \"logo\")\n",
    "                track_logo_bp_mse.append(lmvae.pred.metrics.bp_mse.features.mean())\n",
    "                track_logo_indus_macf1.append(lmvae.pred.metrics.indus_report['macro avg']['f1-score'])\n",
    "\n",
    "                lmvae.predict(data.test, network = \"des\")\n",
    "                track_des_bin_macf1.append(lmvae.pred.metrics.bin_report['macro avg']['f1-score'])\n",
    "                track_des_cat1_macf1.append(lmvae.pred.metrics.cat1_report['macro avg']['f1-score'])\n",
    "\n",
    "                lmvae.predict(data.test, network = \"mgr\")\n",
    "                track_mgr_bp_mse.append(lmvae.pred.metrics.bp_mse.features.mean())\n",
    "\n",
    "                lmvae.train()\n",
    "    \n",
    "    # Final save of stats\n",
    "    lmvae.eval()\n",
    "    \n",
    "    lmvae.predict(data.training)\n",
    "    lmvae.pred.metrics.summarize(path = str(save_index) + \"_training_metrics.csv\", index = fold, givens = givens)\n",
    "    lmvae.pred.metrics.save_features_table(path = str(save_index) + \"_training_bin_features.csv\", names = binfeats.columns, index = fold, givens = givens)\n",
    "    lmvae.pred.ll.summarize(path = str(save_index) + \"_training_ll.csv\", index = fold, givens = givens)\n",
    "        \n",
    "    if hasattr(data, 'test'):\n",
    "        data.test.make_torch()\n",
    "        lmvae.predict(data.test)\n",
    "        lmvae.pred.metrics.summarize(path = str(save_index) + \"_test_metrics.csv\", index = fold, givens = givens)\n",
    "        lmvae.pred.metrics.save_features_table(path = str(save_index) + \"_test_bin_features.csv\", names = binfeats.columns, index = fold, givens = givens)\n",
    "        lmvae.pred.ll.summarize(path = str(save_index) + \"_test_ll.csv\", index = fold, givens = givens)\n",
    "        \n",
    "        lmvae.predict(data.test, network = \"logo\")\n",
    "        lmvae.pred.metrics.summarize(path = str(save_index) + \"_logo_metrics.csv\", index = fold, givens = givens)\n",
    "        lmvae.pred.ll.summarize(path = str(save_index) + \"_logo_ll.csv\", index = fold, givens = givens)\n",
    "                \n",
    "        lmvae.predict(data.test, network = \"des\")\n",
    "        lmvae.pred.metrics.summarize(path = str(save_index) + \"_des_metrics.csv\", index = fold, givens = givens)\n",
    "        lmvae.pred.metrics.save_features_table(path = str(save_index) + \"_des_bin_features.csv\", names = binfeats.columns, index = fold, givens = givens)\n",
    "        lmvae.pred.ll.summarize(path = str(save_index) + \"_des_ll.csv\", index = fold, givens = givens)\n",
    "                \n",
    "        lmvae.predict(data.test, network = \"mgr\")\n",
    "        lmvae.pred.metrics.summarize(path = str(save_index) + \"_mgr_metrics.csv\", index = fold, givens = givens)\n",
    "        lmvae.pred.ll.summarize(path = str(save_index) + \"_mgr_ll.csv\", index = fold, givens = givens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(track_logo_bp_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([-x for x in track_logo_indus_macf1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([-x for x in track_des_bin_macf1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([-x for x in track_des_cat1_macf1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(track_mgr_bp_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmvae.predict(data.training)\n",
    "\n",
    "z = lmvae.pred.z.z_loc.cpu().numpy()\n",
    "end_names = data.training.names\n",
    "# z_est = z_est[:,z_est.std(0) > 0.5]\n",
    "\n",
    "dist_z = compute_distance(z)\n",
    "\n",
    "test_firms = ['itw','harman-intl','lilly','goldman-sachs','21st-century-fox','facebook','gucci','old-navy','3m','actavis','mcdonalds', 'kfc']\n",
    "test_neighbors = [end_names[dist_z[np.where(end_names == test_firms[i])[0][0],:].argsort()][1:5] for i in range(len(test_firms))]\n",
    "test_dist = [np.sort(dist_z[np.where(end_names == test_firms[i])[0][0],:].round(2))[1:5] for i in range(len(test_firms))]\n",
    "formatted_neighbors = [\", \".join(test_neighbors[i].tolist()) for i in range(len(test_neighbors))]\n",
    "\n",
    "neighbors_df = pd.DataFrame(test_neighbors)\n",
    "neighbors_df.index = test_firms\n",
    "neighbors_df.columns = np.arange(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store().save(\"lmvae_20_100_lowlr_full.pyro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lmvae.state_dict(), \"lmvae_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Predict\n",
    "\n",
    "class PredictNoVar():\n",
    "    def __init__(self, lmvae, Z):\n",
    "        bp = lmvae.bp_decoder(Z.cuda())\n",
    "        self.bp = bp[0].cpu().detach()\n",
    "        self.bin = lmvae.bin_decoder(Z.cuda()).cpu().detach()\n",
    "        self.indus = lmvae.indus_decoder(Z.cuda()).cpu().detach()\n",
    "        self.text = lmvae.text_decoder(Z.cuda()).cpu().detach()\n",
    "        self.cat1 = lmvae.cat1_decoder(Z.cuda()).cpu().detach()\n",
    "        self.cat2 = lmvae.cat2_decoder(Z.cuda()).cpu().detach()\n",
    "        self.cat3 = lmvae.cat3_decoder(Z.cuda()).cpu().detach()\n",
    "        self.cat4 = lmvae.cat4_decoder(Z.cuda()).cpu().detach()\n",
    "        self.cat5 = lmvae.cat5_decoder(Z.cuda()).cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomBrand():\n",
    "    def __init__(self, lmvae, K, N = 100):\n",
    "        self.K = K\n",
    "        self.N = N\n",
    "        self.Z = dist.Normal(loc=torch.tensor(0.), scale=torch.tensor(1.)).sample([N,self.K])\n",
    "        self.pred = PredictNoVar(lmvae, self.Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_profile(pred, i = 0):\n",
    "\n",
    "    return {'bp': pred.bp[i],\n",
    "            'bin': pred.bin[i],\n",
    "            'indus': pred.indus[i],\n",
    "            'text': pred.text[i],\n",
    "            'cat1': pred.cat1[i],\n",
    "            'cat2': pred.cat2[i],\n",
    "            'cat3': pred.cat3[i],\n",
    "            'cat4': pred.cat4[i],\n",
    "            'cat5': pred.cat5[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile(pred, data, i = 0):\n",
    "\n",
    "    raw = raw_profile(pred, i)\n",
    "\n",
    "    # Binary logo feats:\n",
    "    act_probs = pd.DataFrame(raw[\"bin\"])\n",
    "    act_probs.index = binfeats.columns\n",
    "    act_probs.columns = [\"Prob\"]\n",
    "\n",
    "    rel_probs = pd.DataFrame(raw[\"bin\"] - data.bin.mean(0).cpu().numpy())\n",
    "    rel_probs.index = binfeats.columns\n",
    "    rel_probs.columns = [\"Rel Prob\"]\n",
    "\n",
    "    bin_profile = pd.concat([rel_probs, act_probs], axis=1)\n",
    "    bin_profile = bin_profile.sort_values(by=\"Rel Prob\", ascending=False)\n",
    "\n",
    "    # Cat 1:\n",
    "    act_probs = pd.DataFrame(raw[\"cat1\"])\n",
    "    act_probs.index = c1_labels\n",
    "    act_probs.columns = [\"Prob\"]\n",
    "\n",
    "    c1_probs = pd.Series(data.cat1.cpu().numpy().flatten()).value_counts().sort_index().to_numpy() / data.cat1.shape[0]\n",
    "\n",
    "    rel_probs = pd.DataFrame(raw[\"cat1\"] - c1_probs)\n",
    "    rel_probs.index = c1_labels\n",
    "    rel_probs.columns = [\"Rel Prob\"]\n",
    "\n",
    "    cat1_profile = pd.concat([rel_probs, act_probs], axis=1)\n",
    "    cat1_profile = cat1_profile.sort_values(by=\"Rel Prob\", ascending=False)\n",
    "\n",
    "    # Cat 2:\n",
    "    act_probs = pd.DataFrame(raw[\"cat2\"])\n",
    "    act_probs.index = c2_labels\n",
    "    act_probs.columns = [\"Prob\"]\n",
    "\n",
    "    c2_probs = pd.Series(data.cat2.cpu().numpy().flatten()).value_counts().sort_index().to_numpy() / data.cat2.shape[0]\n",
    "\n",
    "    rel_probs = pd.DataFrame(raw[\"cat2\"] - c2_probs)\n",
    "    rel_probs.index = c2_labels\n",
    "    rel_probs.columns = [\"Rel Prob\"]\n",
    "\n",
    "    cat2_profile = pd.concat([rel_probs, act_probs], axis=1)\n",
    "    cat2_profile = cat2_profile.sort_values(by=\"Rel Prob\", ascending=False)\n",
    "\n",
    "    # Cat 3:\n",
    "    act_probs = pd.DataFrame(raw[\"cat3\"])\n",
    "    act_probs.index = c3_labels\n",
    "    act_probs.columns = [\"Prob\"]\n",
    "\n",
    "    c3_probs = pd.Series(data.cat3.cpu().numpy().flatten()).value_counts().sort_index().to_numpy() / data.cat3.shape[0]\n",
    "\n",
    "    rel_probs = pd.DataFrame(raw[\"cat3\"] - c3_probs)\n",
    "    rel_probs.index = c3_labels\n",
    "    rel_probs.columns = [\"Rel Prob\"]\n",
    "\n",
    "    cat3_profile = pd.concat([rel_probs, act_probs], axis=1)\n",
    "    cat3_profile = cat3_profile.sort_values(by=\"Rel Prob\", ascending=False)\n",
    "\n",
    "    # Cat 4:\n",
    "    act_probs = pd.DataFrame(raw[\"cat4\"])\n",
    "    act_probs.index = c4_labels\n",
    "    act_probs.columns = [\"Prob\"]\n",
    "\n",
    "    c4_probs = pd.Series(data.cat4.cpu().numpy().flatten()).value_counts().sort_index().to_numpy() / data.cat4.shape[0]\n",
    "\n",
    "    rel_probs = pd.DataFrame(raw[\"cat4\"] - c4_probs)\n",
    "    rel_probs.index = c4_labels\n",
    "    rel_probs.columns = [\"Rel Prob\"]\n",
    "\n",
    "    cat4_profile = pd.concat([rel_probs, act_probs], axis=1)\n",
    "    cat4_profile = cat4_profile.sort_values(by=\"Rel Prob\", ascending=False)\n",
    "\n",
    "    # Cat 5:\n",
    "    act_probs = pd.DataFrame(raw[\"cat5\"])\n",
    "    act_probs.index = c5_labels\n",
    "    act_probs.columns = [\"Prob\"]\n",
    "\n",
    "    c5_probs = pd.Series(data.cat5.cpu().numpy().flatten()).value_counts().sort_index().to_numpy() / data.cat5.shape[0]\n",
    "\n",
    "    rel_probs = pd.DataFrame(raw[\"cat5\"] - c5_probs)\n",
    "    rel_probs.index = c5_labels\n",
    "    rel_probs.columns = [\"Rel Prob\"]\n",
    "\n",
    "    cat5_profile = pd.concat([rel_probs, act_probs], axis=1)\n",
    "    cat5_profile = cat5_profile.sort_values(by=\"Rel Prob\", ascending=False)\n",
    "\n",
    "    # Indus tags:\n",
    "    act_probs = pd.DataFrame(raw[\"indus\"])\n",
    "    act_probs.index = indus_labels\n",
    "    act_probs.columns = [\"Prob\"]\n",
    "\n",
    "    rel_probs = pd.DataFrame(raw[\"indus\"] - data.indus.mean(0).cpu().numpy())\n",
    "    rel_probs.index = indus_labels\n",
    "    rel_probs.columns = [\"Rel Prob\"]\n",
    "\n",
    "    indus_profile = pd.concat([rel_probs, act_probs], axis=1)\n",
    "    indus_profile = indus_profile.sort_values(by=\"Rel Prob\", ascending=False)\n",
    "\n",
    "    # BP:\n",
    "    bp_profile = pd.DataFrame(raw[\"bp\"])\n",
    "    bp_profile.index = bp_labels\n",
    "    bp_profile.columns = [\"Rel Values\"]\n",
    "    bp_profile = bp_profile.sort_values(by=\"Rel Values\", ascending=False)\n",
    "\n",
    "    # Text:\n",
    "    act_probs = pd.DataFrame(raw[\"text\"])\n",
    "    act_probs.index = words\n",
    "    act_probs.columns = [\"Prob\"]\n",
    "\n",
    "    rel_probs = pd.DataFrame(raw[\"text\"] - data.text.mean(0).cpu().numpy())\n",
    "    rel_probs.index = words\n",
    "    rel_probs.columns = [\"Rel Prob\"]\n",
    "\n",
    "    text_profile = pd.concat([rel_probs, act_probs], axis=1)\n",
    "    text_profile = text_profile.sort_values(by=\"Rel Prob\", ascending=False)\n",
    "\n",
    "\n",
    "    return {\"bp\": bp_profile, \"text\": text_profile, \"indus\": indus_profile,\n",
    "            \"bin\": bin_profile, \"cat1\": cat1_profile, \"cat2\": cat2_profile,\n",
    "            \"cat3\": cat3_profile, \"cat4\": cat4_profile, \"cat5\": cat5_profile}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testgen = RandomBrand(lmvae, K, N = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testprof = profile(testgen.pred, data = data.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testprof['bp'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testprof['bp'][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testprof['indus'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brand Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We don't actually need this:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanyData():\n",
    "    pass\n",
    "\n",
    "def get_company(data, index=None, name=None, cuda=False):\n",
    "    if (index == None) and (name == None):\n",
    "        raise Exception(\"Need either an index or a name\")\n",
    "\n",
    "    if (index != None) and (name != None):\n",
    "        raise Exception(\"Can't have both an index and a name\")\n",
    "\n",
    "    company = CompanyData()\n",
    "    if (index != None):\n",
    "        company.text = torch.tensor(data.x_text[index], dtype = torch.float)\n",
    "        company.bin = torch.tensor(data.x_bin[index], dtype = torch.float)\n",
    "        company.cat1 = torch.tensor(data.x_cat1[index], dtype = torch.float)\n",
    "        company.cat2 = torch.tensor(data.x_cat2[index], dtype = torch.float)\n",
    "        company.cat3 = torch.tensor(data.x_cat3[index], dtype = torch.float)\n",
    "        company.cat4 = torch.tensor(data.x_cat4[index], dtype = torch.float)\n",
    "        company.cat5 = torch.tensor(data.x_cat5[index], dtype = torch.float)\n",
    "        company.bp = torch.tensor(data.x_bp[index], dtype = torch.float)\n",
    "        company.indus = torch.tensor(data.x_indus[index], dtype = torch.float)\n",
    "\n",
    "    if (name != None):\n",
    "        company.text = torch.tensor(data.x_text[data.x_names == name], dtype = torch.float)\n",
    "        company.bin = torch.tensor(data.x_bin[data.x_names == name], dtype = torch.float)\n",
    "        company.cat1 = torch.tensor(data.x_cat1[data.x_names == name], dtype = torch.float)\n",
    "        company.cat2 = torch.tensor(data.x_cat2[data.x_names == name], dtype = torch.float)\n",
    "        company.cat3 = torch.tensor(data.x_cat3[data.x_names == name], dtype = torch.float)\n",
    "        company.cat4 = torch.tensor(data.x_cat4[data.x_names == name], dtype = torch.float)\n",
    "        company.cat5 = torch.tensor(data.x_cat5[data.x_names == name], dtype = torch.float)\n",
    "        company.bp = torch.tensor(data.x_bp[data.x_names == name], dtype = torch.float)\n",
    "        company.indus = torch.tensor(data.x_indus[data.x_names == name], dtype = torch.float)\n",
    "        \n",
    "    if cuda:\n",
    "        company.text = company.text.cuda()\n",
    "        company.bin = company.bin.cuda()\n",
    "        company.cat1 = company.cat1.cuda()\n",
    "        company.cat2 = company.cat2.cuda()\n",
    "        company.cat3 = company.cat3.cuda()\n",
    "        company.cat4 = company.cat4.cuda()\n",
    "        company.cat5 = company.cat5.cuda()\n",
    "        company.indus = company.indus.cuda()\n",
    "        company.bp = company.bp.cuda()\n",
    "\n",
    "    return company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_mckinsey = get_company(data.training, name = \"mckinsey\", cuda = True)\n",
    "x_goldman = get_company(data.training, name = \"goldman-sachs\", cuda = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_goldman.bp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brand Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmvae.predict(data.training)\n",
    "\n",
    "z = lmvae.pred.z.z_loc.cpu().numpy()\n",
    "end_names = data.training.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp(logic1, logic2, n=10, w1=0.5, w2=0.5, return_z = False):\n",
    "    interp = w1 * z[logic1].mean(0) + w2 * z[logic2].mean(0)\n",
    "    interp_dists = compute_distance(np.vstack([z, interp]))\n",
    "    if return_z:\n",
    "        return interp\n",
    "    else: \n",
    "        return end_names[interp_dists[-1,:].argsort()[1:(n+1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp(end_names == \"goldman-sachs\", end_names == \"mckinsey\", n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp(end_names == \"nike\", end_names == \"gucci\", n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health + {Tech, Finance}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_company = data.training.indus.cpu().numpy()[:,np.isin(indus_labels, [\"Hardware\",\"Consumer.Electronics\",\"Software\"])].max(1) == 1\n",
    "health_company = data.training.indus.cpu().numpy()[:,np.isin(indus_labels, [\"Health.Care\"])].max(1) == 1\n",
    "finance_company = data.training.indus.cpu().numpy()[:,np.isin(indus_labels, [\"Financial.Services\"])].max(1) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interp(tech_company, health_company, n=10, w1=3, w2=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp(finance_company, health_company, w1=3, w2=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shopping + {Data, Payments}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payments_company = data.training.indus.cpu().numpy()[:,np.isin(indus_labels, [\"Payments\"])].max(1) == 1\n",
    "shopping_company = data.training.indus.cpu().numpy()[:,np.isin(indus_labels, [\"Commerce.and.Shopping\"])].max(1) == 1\n",
    "data_company = data.training.indus.cpu().numpy()[:,np.isin(indus_labels, [\"Data.and.Analytics\"])].max(1) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp(payments_company, shopping_company, w1=3, w2=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp(shopping_company, data_company, w1=3, w2=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daring Fast Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_daring_ff = interp(daring_company, fastfood_company, w2 = 3, w1 = 1)\n",
    "z_daring_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daring_company = data.training.bp.cpu().numpy()[:, bp.columns == \"daring\"].flatten() > 2.\n",
    "fastfood_company = np.isin(end_names, [\"mcdonalds\",\"burger-king\",\"kfc\"])\n",
    "\n",
    "z_daring_ff = interp(daring_company, fastfood_company, w1 = 2, w2 = 1, return_z = True)\n",
    "\n",
    "pred_daring_ff = PredictNoVar(lmvae, torch.tensor(z_daring_ff).unsqueeze(0))\n",
    "prof_daring_ff = profile(pred_daring_ff, data.training, i=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_daring_ff['bp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McDonald's Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewCompany(CompanyData):\n",
    "    def __init__(self, name, read_dir = \"../../code/extract_features/new_logo_outputs/\"):\n",
    "    \n",
    "        self.bp = pd.read_csv(read_dir + name + \"_rel_bp.csv\", header=None, index_col=0).values.T\n",
    "\n",
    "        indus_df = pd.read_csv(read_dir + name + \"_indus.csv\", header=None, index_col=0)\n",
    "        self.indus = indus_df.values.T\n",
    "\n",
    "        new_bin = pd.read_csv(read_dir + name + \"_y_bin.csv\", index_col=0)\n",
    "        self.bin = new_bin.values\n",
    "\n",
    "        new_mult = pd.read_csv(read_dir + name + \"_y_mult.csv\", index_col=0)\n",
    "\n",
    "        self.cat1 = np.expand_dims(new_mult.values[:,0], 1)\n",
    "        self.cat2 = np.expand_dims(new_mult.values[:,1], 1)\n",
    "        self.cat3 = np.expand_dims(new_mult.values[:,2], 1)\n",
    "        self.cat4 = np.expand_dims(new_mult.values[:,3], 1)\n",
    "        self.cat5 = np.expand_dims(new_mult.values[:,4], 1)\n",
    "\n",
    "        new_text_df = pd.read_csv(read_dir + name + \"_newrow_binary.csv\", index_col=0)\n",
    "        self.text = new_text_df.values\n",
    "        \n",
    "    def make_torch(self, cuda = False):\n",
    "        if cuda:\n",
    "            self.text = torch.tensor(self.text, dtype = torch.float).cuda()\n",
    "            self.bin = torch.tensor(self.bin, dtype = torch.float).cuda()\n",
    "            self.cat1 = torch.tensor(self.cat1, dtype = torch.float).cuda()\n",
    "            self.cat2 = torch.tensor(self.cat2, dtype = torch.float).cuda()\n",
    "            self.cat3 = torch.tensor(self.cat3, dtype = torch.float).cuda()\n",
    "            self.cat4 = torch.tensor(self.cat4, dtype = torch.float).cuda()\n",
    "            self.cat5 = torch.tensor(self.cat5, dtype = torch.float).cuda()\n",
    "            self.bp = torch.tensor(self.bp, dtype = torch.float).cuda()\n",
    "            self.indus = torch.tensor(self.indus, dtype = torch.float).cuda()\n",
    "        else:\n",
    "            self.text = torch.tensor(self.text, dtype = torch.float)\n",
    "            self.bin = torch.tensor(self.bin, dtype = torch.float)\n",
    "            self.cat1 = torch.tensor(self.cat1, dtype = torch.float)\n",
    "            self.cat2 = torch.tensor(self.cat2, dtype = torch.float)\n",
    "            self.cat3 = torch.tensor(self.cat3, dtype = torch.float)\n",
    "            self.cat4 = torch.tensor(self.cat4, dtype = torch.float)\n",
    "            self.cat5 = torch.tensor(self.cat5, dtype = torch.float)\n",
    "            self.bp = torch.tensor(self.bp, dtype = torch.float)\n",
    "            self.indus = torch.tensor(self.indus, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import NewZ, Predict\n",
    "\n",
    "new_mcds1 = NewCompany(name = \"mcdonalds1\", read_dir = \"../../../extract_features/new_logo_outputs/\")\n",
    "new_mcds1.make_torch(cuda = True)\n",
    "z_mcds1 = NewZ(lmvae, data = new_mcds1, network = \"mgr\")\n",
    "\n",
    "new_mcds2 = NewCompany(name = \"mcdonalds2\", read_dir = \"../../../extract_features/new_logo_outputs/\")\n",
    "new_mcds2.make_torch(cuda = True)\n",
    "z_mcds2 = NewZ(lmvae, data = new_mcds2, network = \"mgr\")\n",
    "\n",
    "new_mcds0 = NewCompany(name = \"mcdonalds-old\", read_dir = \"../../../extract_features/new_logo_outputs/\")\n",
    "new_mcds0.make_torch(cuda = True)\n",
    "z_mcds0 = NewZ(lmvae, data = new_mcds0, network = \"mgr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mcds1 = Predict(lmvae, z = z_mcds1)\n",
    "pred_mcds2 = Predict(lmvae, z = z_mcds2)\n",
    "pred_mcds0 = Predict(lmvae, z = z_mcds0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mcds1.bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_mcds2.bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_mcds0.bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(np.vstack([bp.columns, pred_mcds1.bp, pred_mcds2.bp, pred_mcds0.bp]).T)\n",
    "out.to_csv(\"../../../mcdonalds/new_model_mcd_bp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shake Shack / In-n-Out Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiviewZ():\n",
    "    def __init__(self, lmvae, data):\n",
    "        self.full = NewZ(lmvae, data, network = \"full\")\n",
    "        self.mgr = NewZ(lmvae, data, network = \"mgr\")\n",
    "        self.des = NewZ(lmvae, data, network = \"des\")\n",
    "        self.logo = NewZ(lmvae, data, network = \"logo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ss = NewCompany(name = \"shake-shack\", read_dir = \"../../../extract_features/new_logo_outputs/\")\n",
    "data_ss.make_torch(cuda = True)\n",
    "z_ss = MultiviewZ(lmvae, data_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(z_ss.full.z_loc.cpu().numpy(), z_ss.mgr.z_loc.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(z_ss.full.z_loc.cpu().numpy(), z_ss.logo.z_loc.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(z_ss.full.z_loc.cpu().numpy(), z_ss.des.z_loc.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ino = NewCompany(name = \"in-n-out\", read_dir = \"../../../extract_features/new_logo_outputs/\")\n",
    "data_ino.make_torch(cuda = True)\n",
    "z_ino = MultiviewZ(lmvae, data_ino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(z_ss.full.z_loc.cpu(), z_ino.full.z_loc.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ss = Predict(lmvae, z_ss.des)\n",
    "prof_ss = profile(pred_ss, data.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prof_ss[\"bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ino = Predict(lmvae, z_ino.des)\n",
    "prof_ino = profile(pred_ino, data.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_ino[\"bin\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro",
   "language": "python",
   "name": "pyro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
