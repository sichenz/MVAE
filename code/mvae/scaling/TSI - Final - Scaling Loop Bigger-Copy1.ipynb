{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "save_index = str('x'+datetime.now().strftime(\"%m%d%y-%H%M%S\"))\n",
    "\n",
    "import os\n",
    "os.mkdir(save_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECODER_DIMS = {\"text\": 400, \"logo\": 400, \"bp\": 400, \"indus\": 400}\n",
    "ENCODER_DIMS = {\"full\": 400, \"res\": 50, \"mgr\": 200, \"design\": 200}\n",
    "K = 20\n",
    "\n",
    "FOLDS = 4\n",
    "BATCHES = 5000\n",
    "ITERS = 10\n",
    "\n",
    "ADAM_LR = 1e-5\n",
    "MIN_AF = 1e-6\n",
    "ANNEALING_BATCHES = 4000\n",
    "NUM_PARTICLES = 1\n",
    "\n",
    "CENTER_BP = True\n",
    "\n",
    "DISABLE_TQDM = False\n",
    "\n",
    "WEIGHT_DECAY = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.examples.util  # patches torchvision\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro import poutine\n",
    "\n",
    "pyro.set_rng_seed(42)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from skimage import io\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from data import SplitData\n",
    "from model import LogoMVAE\n",
    "\n",
    "assert pyro.__version__.startswith('1.3.0')\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "\n",
    "def compute_distance(z):\n",
    "    b = z.reshape(z.shape[0], 1, z.shape[1])\n",
    "    return np.sqrt(np.einsum('ijk, ijk->ij', z-b, z-b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "First, load text data, and apply word filter. Note on notation: `tx` stands for \"true x,\" because the model variables are also called x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf = pd.read_csv(\"../../../data/web_dtfm20_binary.csv\", index_col=0)\n",
    "tx_text = textdf.values\n",
    "seltext = tx_text.sum(0) > 0.05\n",
    "tx_text = textdf.values[:,seltext]\n",
    "\n",
    "gt20words = tx_text.sum(1) > 20\n",
    "tx_text = tx_text[gt20words,:]\n",
    "\n",
    "words = textdf.columns[seltext]\n",
    "N, V = tx_text.shape\n",
    "\n",
    "binfeats = pd.read_csv(\"../../../data/y_bin_all_py2.csv\", index_col=0)\n",
    "tx_b = binfeats.values\n",
    "tx_b = tx_b[gt20words,:]\n",
    "M_b = tx_b.shape[1]\n",
    "\n",
    "catfeats = pd.read_csv(\"../../../data/y_mult_ncolors_py2.csv\", index_col=0)\n",
    "\n",
    "tx_c1 = catfeats.values[:,0][gt20words]\n",
    "M_c1 = len(np.unique(tx_c1))\n",
    "tx_c1 = np.expand_dims(tx_c1, 1)\n",
    "\n",
    "tx_c2 = catfeats.values[:,1][gt20words]\n",
    "M_c2 = len(np.unique(tx_c2))\n",
    "tx_c2 = np.expand_dims(tx_c2, 1)\n",
    "\n",
    "tx_c3 = catfeats.values[:,2][gt20words]\n",
    "M_c3 = len(np.unique(tx_c3))\n",
    "tx_c3 = np.expand_dims(tx_c3, 1)\n",
    "\n",
    "tx_c4 = catfeats.values[:,3][gt20words]\n",
    "M_c4 = len(np.unique(tx_c4))\n",
    "tx_c4 = np.expand_dims(tx_c4, 1)\n",
    "\n",
    "tx_c5 = catfeats.values[:,4][gt20words]\n",
    "M_c5 = len(np.unique(tx_c5))\n",
    "tx_c5 = np.expand_dims(tx_c5, 1)\n",
    "\n",
    "c1_labels = np.array([\"black\",\"blue_dark\",\"blue_light\",\"blue_medium\",\"brown\",\"green_dark\",\n",
    "                      \"green_light\",\"grey_dark\",\"grey_light\",\"orange\",\"red\",\"red_dark\",\n",
    "                      \"yellow\"])\n",
    "\n",
    "c2_labels = np.array([\"circle\",\"rect-oval_medium\",\"rect-oval_large\",\"rect-oval_thin\",\n",
    "                      \"square\",\"triangle\"])\n",
    "\n",
    "c3_labels = np.array([\"bad_letters\",\"bulky_hollow_geometric\",\"circular\",\"dense_simple_geom\",\n",
    "                      \"detailed_circle\",\"hollow_circle\",\"detailed_hor\",\"long_hor\",\"no_mark\",\n",
    "                      \"simple\",\"square\",\"thin_vert_rect\",\"vert_narrow\",\"detailed\",\"thin\",\n",
    "                      \"hor_wispy\"])\n",
    "\n",
    "c4_labels = np.array([\"nochars\",\"sans\",\"serif\"])\n",
    "\n",
    "c5_labels = np.array([\"one_color\",\"two_colors\",\"three_colors\",\"many_colors\"])\n",
    "\n",
    "bp = pd.read_csv(\"../../../data/bp_avg_all_traits.csv\", index_col=0)\n",
    "\n",
    "bp_labels = bp.columns\n",
    "\n",
    "tx_bp = bp.values\n",
    "tx_bp = tx_bp[gt20words]\n",
    "if CENTER_BP:\n",
    "    tx_bp = (tx_bp - tx_bp.mean(0)) / tx_bp.std(0)\n",
    "M_bp = tx_bp.shape[1]\n",
    "\n",
    "indus = pd.read_csv(\"../../../data/industry_codes_b2bc.csv\", index_col=0)\n",
    "indus = indus.iloc[np.in1d(indus.index, bp.index),:]\n",
    "indus = indus.sort_index()\n",
    "\n",
    "tx_indus = indus.values.astype('int')\n",
    "tx_indus = tx_indus[:, tx_indus.sum(0) > 9]\n",
    "tx_indus = tx_indus[gt20words,:]\n",
    "M_indus = tx_indus.shape[1]\n",
    "\n",
    "indus_labels = indus.columns[indus.values.sum(0) > 9]\n",
    "\n",
    "allnames = binfeats.index.values[gt20words]\n",
    "\n",
    "x_sizes = {\"text\": V, \n",
    "           \"bin\": M_b, \n",
    "           \"cat1\": M_c1, \n",
    "           \"cat2\": M_c2, \n",
    "           \"cat3\": M_c3, \n",
    "           \"cat4\": M_c4, \n",
    "           \"cat5\": M_c5, \n",
    "           \"bp\": M_bp, \n",
    "           \"indus\": M_indus, \n",
    "           \"logo\": M_b + M_c1 + M_c2 + M_c3 + M_c4 + M_c5, \n",
    "           \"all\": V + M_b + M_c1 + M_c2 + M_c3 + M_c4 + M_c5 + M_bp + M_indus}\n",
    "\n",
    "task_sizes = {\"full\": x_sizes[\"all\"], \n",
    "              \"res\": x_sizes[\"logo\"] + x_sizes[\"indus\"], \n",
    "              \"design\": x_sizes[\"text\"] + x_sizes[\"bp\"] + x_sizes[\"indus\"], \n",
    "              \"mgr\": x_sizes[\"all\"] - x_sizes[\"bp\"]}\n",
    "\n",
    "noptions = np.array([M_c1, M_c2, M_c3, M_c4, M_c5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: Instantiate Model and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "givens = pd.DataFrame(np.concatenate(([[K], list(DECODER_DIMS.values()), list(ENCODER_DIMS.values()), [BATCHES], [ITERS], [ADAM_LR], [ANNEALING_BATCHES], [NUM_PARTICLES], [CENTER_BP], [WEIGHT_DECAY], [FOLDS]]))).T\n",
    "givens.columns = [\"K\", \"text_dec\", \"logo_dec\", \"bp_dec\", \"indus_dec\", \"full_enc\", \"res_enc\", \"mgr_enc\", \"des_enc\", \"batches\", \"iters\", \"adam_lr\", \"annealing_batches\", \"num_particles\", \"center_bp\", \"weight_decay\",\"folds\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create holdout and cross-validation subsets (just the indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FOLDS > 1:\n",
    "    holdout_indices = list(split(np.arange(N), FOLDS))\n",
    "    holdout_indices.append(np.array([]))\n",
    "    fold_indices = [np.setdiff1d(np.arange(N), holdout_indices[i]) for i in range(FOLDS)]\n",
    "    fold_indices.append(np.arange(N))\n",
    "else:\n",
    "    holdout_indices = [np.array([])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the KL annealing schedule (same across each fold):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = np.linspace(MIN_AF, 1., ANNEALING_BATCHES)\n",
    "# schedule = np.concatenate([np.linspace(MIN_AF, 1., round(ANNEALING_BATCHES/4.)) for _ in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_everything = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bdfcea475a34c6a878fd4278b8ec743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=5000.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=5000.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=5000.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=5000.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=5000.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=5000.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=5000.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=5000.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=5000.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=5000.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=5000.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=5000.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cbefb7318b4792807d90e6fcd66904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=5000.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the model across all folds (sequentially):\n",
    "\n",
    "fold = FOLDS\n",
    "\n",
    "for trial in tqdm(range(10)):\n",
    "    track_z = dict()\n",
    "    track_neighbors = dict()\n",
    "    track_names = dict()\n",
    "\n",
    "    for scale_zero in [\"text\",\"logo\",\"bp\",\"indus\",\"full\",\"full2\"]:\n",
    "\n",
    "        domain_scaling = {\"text\": 1., \n",
    "                          \"logo\": 1., \n",
    "                          \"bp\": 1., \n",
    "                          \"indus\": 1.}\n",
    "\n",
    "        if scale_zero is not \"full\" and scale_zero is not \"full2\":\n",
    "            domain_scaling[scale_zero] = 1e-8\n",
    "\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        data = SplitData(tx_text, tx_b, tx_c1, tx_c2, tx_c3, tx_c4, tx_c5, tx_bp, tx_indus, \n",
    "                         allnames, noptions, test_indices = holdout_indices[fold])   \n",
    "\n",
    "        lmvae = LogoMVAE(K, ENCODER_DIMS, DECODER_DIMS, x_sizes, task_sizes, use_cuda = True, domain_scaling = domain_scaling)\n",
    "        optimizer = Adam({\"lr\": ADAM_LR}) #, \"weight_decay\": 0.4})\n",
    "        svi = SVI(lmvae.model, lmvae.guide, optimizer, loss=Trace_ELBO(num_particles = NUM_PARTICLES))\n",
    "\n",
    "        for i in tqdm(range(BATCHES), desc=\"Batches\", leave=False, disable=DISABLE_TQDM):\n",
    "\n",
    "            if i < ANNEALING_BATCHES:\n",
    "                annealing_factor = schedule[i]\n",
    "            else:\n",
    "                annealing_factor = 1.\n",
    "\n",
    "            data.training.shuffle()\n",
    "\n",
    "            for j in tqdm(range(ITERS), desc=\"Iters\", leave=False, disable=True):\n",
    "                svi.step(data.training, annealing_factor)\n",
    "\n",
    "        # Final save of stats\n",
    "        lmvae.eval()\n",
    "\n",
    "        lmvae.predict(data.training)\n",
    "\n",
    "        z = lmvae.pred.z.z_loc.cpu().numpy()\n",
    "        end_names = data.training.names\n",
    "        # z_est = z_est[:,z_est.std(0) > 0.5]\n",
    "\n",
    "        dist_z = compute_distance(z)\n",
    "\n",
    "        test_firms = ['itw','harman-intl','lilly','goldman-sachs','21st-century-fox','facebook','gucci','old-navy','3m','actavis','mcdonalds', 'kfc']\n",
    "        test_neighbors = [end_names[dist_z[np.where(end_names == test_firms[i])[0][0],:].argsort()][1:5] for i in range(len(test_firms))]\n",
    "        test_dist = [np.sort(dist_z[np.where(end_names == test_firms[i])[0][0],:].round(2))[1:5] for i in range(len(test_firms))]\n",
    "        formatted_neighbors = [\", \".join(test_neighbors[i].tolist()) for i in range(len(test_neighbors))]\n",
    "\n",
    "        neighbors_df = pd.DataFrame(test_neighbors)\n",
    "        neighbors_df.index = test_firms\n",
    "        neighbors_df.columns = np.arange(1,5)\n",
    "\n",
    "        track_z[scale_zero] = z\n",
    "        track_names[scale_zero] = end_names\n",
    "        track_neighbors[scale_zero] = neighbors_df\n",
    "        \n",
    "    track_everything[trial] = {'z': track_z, 'neighbors': track_neighbors, 'names': track_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tracked_things = track_everything\n",
    "\n",
    "with open(\"track_results_big_redo.dat\", \"wb\") as f:\n",
    "    pickle.dump(tracked_things, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "infile = open(\"track_results_big_redo.dat\",'rb')\n",
    "tracked_things = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(10):\n",
    "    full_z = tracked_things[s]['z']['full']\n",
    "    dist_full = compute_distance(full_z)\n",
    "\n",
    "    top10_full = np.array([np.argsort(dist_full[i])[1:11] for i in range(dist_full.shape[0])])\n",
    "    top10_full_names = tracked_things[s]['names']['full'][top10_full]\n",
    "    top10_full_names_ordered = top10_full_names[np.argsort(tracked_things[s]['names']['full'])]\n",
    "    \n",
    "    full2_z = tracked_things[s]['z']['full2']\n",
    "    dist_full2 = compute_distance(full2_z)\n",
    "\n",
    "    top10_full2 = np.array([np.argsort(dist_full2[i])[1:11] for i in range(dist_full2.shape[0])])\n",
    "    top10_full2_names = tracked_things[s]['names']['full2'][top10_full]\n",
    "    top10_full2_names_ordered = top10_full2_names[np.argsort(tracked_things[s]['names']['full2'])]\n",
    "\n",
    "    no_logo_z = tracked_things[s]['z']['logo']\n",
    "    dist_no_logo = compute_distance(no_logo_z)\n",
    "\n",
    "    top10_no_logo = np.array([np.argsort(dist_no_logo[i])[1:11] for i in range(dist_no_logo.shape[0])])\n",
    "    top10_no_logo_names = tracked_things[s]['names']['logo'][top10_no_logo]\n",
    "    top10_no_logo_names_ordered = top10_no_logo_names[np.argsort(tracked_things[s]['names']['logo'])]\n",
    "\n",
    "    no_text_z = tracked_things[s]['z']['text']\n",
    "    dist_no_text = compute_distance(no_text_z)\n",
    "\n",
    "    top10_no_text = np.array([np.argsort(dist_no_text[i])[1:11] for i in range(dist_no_text.shape[0])])\n",
    "    top10_no_text_names = tracked_things[s]['names']['text'][top10_no_text]\n",
    "    top10_no_text_names_ordered = top10_no_text_names[np.argsort(tracked_things[s]['names']['text'])]\n",
    "\n",
    "    no_bp_z = tracked_things[s]['z']['bp']\n",
    "    dist_no_bp = compute_distance(no_bp_z)\n",
    "\n",
    "    top10_no_bp = np.array([np.argsort(dist_no_bp[i])[1:11] for i in range(dist_no_bp.shape[0])])\n",
    "    top10_no_bp_names = tracked_things[s]['names']['bp'][top10_no_bp]\n",
    "    top10_no_bp_names_ordered = top10_no_bp_names[np.argsort(tracked_things[s]['names']['bp'])]\n",
    "\n",
    "    no_indus_z = tracked_things[s]['z']['indus']\n",
    "    dist_no_indus = compute_distance(no_indus_z)\n",
    "\n",
    "    top10_no_indus = np.array([np.argsort(dist_no_indus[i])[1:11] for i in range(dist_no_indus.shape[0])])\n",
    "    top10_no_indus_names = tracked_things[s]['names']['indus'][top10_no_indus]\n",
    "    top10_no_indus_names_ordered = top10_no_indus_names[np.argsort(tracked_things[s]['names']['indus'])]\n",
    "    \n",
    "    if s == 0:\n",
    "        results = pd.DataFrame({'full2': np.array([np.isin(top10_full2_names_ordered[i], top10_full_names_ordered[i]) for i in range(dist_full2.shape[0])]).sum(1).mean(),\n",
    "                                'logo': np.array([np.isin(top10_no_logo_names_ordered[i], top10_full_names_ordered[i]) for i in range(dist_full.shape[0])]).sum(1).mean(),\n",
    "                                'text': np.array([np.isin(top10_no_text_names_ordered[i], top10_full_names_ordered[i]) for i in range(dist_full.shape[0])]).sum(1).mean(),\n",
    "                                'bp': np.array([np.isin(top10_no_bp_names_ordered[i], top10_full_names_ordered[i]) for i in range(dist_full.shape[0])]).sum(1).mean(),\n",
    "                                'indus': np.array([np.isin(top10_no_indus_names_ordered[i], top10_full_names_ordered[i]) for i in range(dist_full.shape[0])]).sum(1).mean()},\n",
    "                               index = [0])\n",
    "    else:\n",
    "        temp = pd.DataFrame({'full2': np.array([np.isin(top10_full2_names_ordered[i], top10_full_names_ordered[i]) for i in range(dist_full2.shape[0])]).sum(1).mean(),\n",
    "                             'logo': np.array([np.isin(top10_no_logo_names_ordered[i], top10_full_names_ordered[i]) for i in range(dist_full.shape[0])]).sum(1).mean(),\n",
    "                             'text': np.array([np.isin(top10_no_text_names_ordered[i], top10_full_names_ordered[i]) for i in range(dist_full.shape[0])]).sum(1).mean(),\n",
    "                             'bp': np.array([np.isin(top10_no_bp_names_ordered[i], top10_full_names_ordered[i]) for i in range(dist_full.shape[0])]).sum(1).mean(),\n",
    "                             'indus': np.array([np.isin(top10_no_indus_names_ordered[i], top10_full_names_ordered[i]) for i in range(dist_full.shape[0])]).sum(1).mean()},\n",
    "                             index = [s])\n",
    "        results = results.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracked_things[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro",
   "language": "python",
   "name": "pyro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
